{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "#Load the cora dataset\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    g = DGLGraph(data.graph)\n",
    "    return g, features, labels\n",
    "g, features, labels = load_cora_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/huangqi/dgl2/dgl/python/dgl/graph.py:657: UserWarning: to_networkx currently does not support converting node/edge features automatically.\n",
      "  dgl_warning('to_networkx currently does not support converting'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6994106090373281, 1: 0.7949465500485908, 2: 0.9058050383351588, 3: 0.8280479210711769, 4: 0.8291457286432161, 5: 0.7679558011049724, 6: 0.7689969604863222}\n"
     ]
    }
   ],
   "source": [
    "# We first validate that in CORA, intra community connections\n",
    "# are more than inter community connections\n",
    "\n",
    "g_nx = g.to_networkx(g)\n",
    "adj = nx.adjacency_matrix(g_nx).todense()\n",
    "class_to_intra_prob = {i: 0 for i in range(7)}\n",
    "class_to_total = {i: 0 for i in range(7)}\n",
    "class_to_intra_sum = {i: 0 for i in range(7)}\n",
    "for i in range(g_nx.number_of_nodes()):\n",
    "    _, index = np.where(adj[i] !=0)\n",
    "    each_total = len(index)\n",
    "    intra_num = 0\n",
    "    for j in index:\n",
    "        if labels[i] == labels[j]:\n",
    "            intra_num += 1\n",
    "    class_to_total[int(labels[i])] += each_total\n",
    "    class_to_intra_sum[int(labels[i])] += intra_num\n",
    "for i in range(7):\n",
    "    class_to_intra_prob[i] = class_to_intra_sum[i] / class_to_total[i]\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(class_to_intra_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#CORA visualization tool\n",
    "def cora_viz(labels,subgraph):\n",
    "    value = [labels[node] for node in subgraph.nodes()]\n",
    "    pos = nx.spring_layout(subgraph, random_state=1)\n",
    "    nx.draw_networkx(subgraph, \n",
    "                     pos=pos, \n",
    "                     edge_color='k', \n",
    "                     node_size=5, \n",
    "                     cmap=plt.get_cmap('Set2'), \n",
    "                     node_color=value,\n",
    "                     arrows=False,\n",
    "                     width=0.6,\n",
    "                     with_labels=False)   \n",
    "    #plt.show()\n",
    "#cora_viz(labels,g)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-write the dataset into Dataloader\n",
    "from dgl.utils import Index\n",
    "from dgl.batched_graph import batch\n",
    "class CORABinary:\n",
    "    \"\"\"\n",
    "    Simple binary subgraph constrcutor for CORA.\n",
    "    Should be able to be used to extract binary \n",
    "    subgraph from other graph dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    g : DGLGraph object\n",
    "        the graph\n",
    "    features : numpy ndarray\n",
    "        features of nodes\n",
    "    labels : numpy ndarray\n",
    "        labels of nodes\n",
    "    num_classes : int\n",
    "        number of classes at interest\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, g, features, labels, num_classes):\n",
    "        self._g = g\n",
    "        self._features = features\n",
    "        self._labels = labels\n",
    "        self._n_classes = num_classes\n",
    "        self._community_to_node = [[] for i in range(max(labels)+1)]\n",
    "        for node in range(len(labels)):\n",
    "            self._community_to_node[labels[node]].append(node)\n",
    "        self._g_nx = g.to_networkx()\n",
    "        self._subgraphs = []\n",
    "        self._subfeatures = []\n",
    "        self._sublabels = []\n",
    "        \n",
    "        for i,j in itertools.combinations([i for i in range(num_classes)],2):\n",
    "            subgraph, new2oldindex = self.binary_subgraph_cora(self._g_nx, \n",
    "                                                               self._community_to_node, \n",
    "                                                               i, \n",
    "                                                               j)\n",
    "            subfeature = self._features[list(subgraph.nodes()),:]\n",
    "            sublabel = self._labels[list(new2oldindex[i] \n",
    "                                   for i in subgraph.nodes())]\n",
    "            #relabel the subgraph\n",
    "            sublabel[sublabel<=i] = 0\n",
    "            sublabel[sublabel>i] = 1\n",
    "            self._subgraphs.append(DGLGraph(subgraph))\n",
    "            #/TODO: directly build DGL subgraph\n",
    "            self._subfeatures.append(subfeature)\n",
    "            self._sublabels.append(sublabel)\n",
    "        \n",
    "        \n",
    "        #Contruct line graph\n",
    "        \n",
    "        self._line_graphs = [g.line_graph(backtracking=False) \n",
    "                             for g in self._subgraphs]\n",
    "        in_degrees = lambda g: g.in_degrees(\n",
    "                             Index(th.arange(0, g.number_of_nodes()))).unsqueeze(1).float()\n",
    "        #TODO : replace th.arange to F.arange when migrating back to /data\n",
    "        \n",
    "        self._g_degs = [in_degrees(g) for g in self._subgraphs]\n",
    "        self._lg_degs = [in_degrees(lg) for lg in self._line_graphs]\n",
    "        self._pm_pds = list(zip(*[g.edges() for g in self._subgraphs]))[0]\n",
    "        \n",
    "        self._equi_labels = []\n",
    "        for label in self._sublabels:\n",
    "            mirror_label = th.ones(label.shape).long() - label\n",
    "            self._equi_labels.append(mirror_label)\n",
    "    \n",
    "    def binary_subgraph_cora(self,\n",
    "                             original_graph, \n",
    "                             community_to_node, \n",
    "                             classA=0, \n",
    "                             classB=1):\n",
    "        sub = nx.DiGraph(original_graph.subgraph(community_to_node[classA]+\n",
    "                                                 community_to_node[classB]))\n",
    "        #cast to undirected graph to find connected component\n",
    "        sub_und = nx.Graph(sub)\n",
    "        candidate_edge_list = []\n",
    "        for edge in sub.edges():\n",
    "            # Only look at src node of edges that cross communities\n",
    "            if (labels[edge[0]] != labels[edge[1]]):\n",
    "                candidate_edge_list.append(edge)\n",
    "        component_list = []\n",
    "        for edge in candidate_edge_list:\n",
    "            component_list.append(\n",
    "                nx.node_connected_component(sub_und,edge[0]))\n",
    "        component_list.sort(key=len, reverse=True)\n",
    "        \n",
    "        # find the largest connected component to be the candidate subgraph\n",
    "        largest_subgraph = component_list[0]\n",
    "        old2newindex = {}\n",
    "        new2oldindex = {}\n",
    "        reindex_nodes = [i for i in range(len(largest_subgraph))]\n",
    "        reindex_edges = []\n",
    "        for i, node in enumerate(largest_subgraph):\n",
    "            old2newindex[node] = i\n",
    "            new2oldindex[i] = node\n",
    "        for src,dst in sub.edges(largest_subgraph):\n",
    "            reindex_edges.append((old2newindex[src], old2newindex[dst]))\n",
    "\n",
    "        #use old2newindex to manually reindex the subgraph\n",
    "        #/TODO(hq): send nodelist directly to DGLgraph\n",
    "        reindex_graph = nx.DiGraph()\n",
    "        reindex_graph.add_nodes_from(reindex_nodes)\n",
    "        reindex_graph.add_edges_from(reindex_edges)\n",
    "\n",
    "\n",
    "        return reindex_graph, new2oldindex\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._subgraphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._subgraphs[idx], self._line_graphs[idx], self._g_degs[idx], self._lg_degs[idx], self._pm_pds[idx], self._subfeatures[idx], self._sublabels[idx], self._equi_labels[idx]\n",
    "    \n",
    "    def collate_fn(self, x):\n",
    "        subgraph, line_graph, deg_g, deg_lg, pm_pd, subfeature, sublabel, equi_label = zip(*x)\n",
    "        subgraph_batch = batch(subgraph)\n",
    "        line_graph_batch = batch(line_graph)\n",
    "        # TODO : change to F.cat when migrating to /data\n",
    "        deg_g_batch = th.cat(deg_g, dim=0)\n",
    "        deg_lg_batch = th.cat(deg_lg, dim=0)\n",
    "        \n",
    "        self.total = 0\n",
    "        def offset(pm_pd):\n",
    "            prev_total = self.total\n",
    "            self.total += pm_pd.size(0)\n",
    "            return prev_total\n",
    "        pm_pd_batch = th.cat([x + offset(x) for i, x in enumerate(pm_pd)], \n",
    "                             dim=0)\n",
    "        \n",
    "        subfeature_batch = th.cat(subfeature, dim=0)\n",
    "        sublabel_batch = th.cat(sublabel, dim=0)\n",
    "        equilabel_batch = th.cat(equi_label, dim=0)\n",
    "        \n",
    "        return subgraph_batch, line_graph_batch, deg_g_batch, deg_lg_batch, pm_pd_batch,subfeature_batch, sublabel_batch, equilabel_batch    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------Model--------------------------------\n",
    "class GNNModule(nn.Module):\n",
    "    def __init__(self, in_feats, in_y_feats, out_feats, radius):\n",
    "        super().__init__()\n",
    "        self.out_feats = out_feats\n",
    "        self.radius = radius\n",
    "\n",
    "        new_linear = lambda: nn.Linear(in_feats, out_feats)\n",
    "        new_linear_list = lambda: nn.ModuleList([new_linear() for i in range(radius)])\n",
    "        \n",
    "        new_linear_y = lambda: nn.Linear(in_y_feats, out_feats)\n",
    "        new_linear_list_y = lambda: nn.ModuleList([new_linear_y() for i in range(radius)])\n",
    "        self.theta_x, self.theta_deg = \\\n",
    "            new_linear(), new_linear()\n",
    "        self.theta_y = new_linear_y()\n",
    "        self.theta_list = new_linear_list()\n",
    "\n",
    "        self.gamma_deg, self.gamma_x = \\\n",
    "            new_linear(), new_linear()\n",
    "        self.gamma_y = new_linear_y()\n",
    "        self.gamma_list = new_linear_list_y()\n",
    "\n",
    "        self.bn_x = nn.BatchNorm1d(out_feats)\n",
    "        self.bn_y = nn.BatchNorm1d(out_feats)\n",
    "\n",
    "    def aggregate(self, g, z):\n",
    "        z_list = []\n",
    "        g.set_n_repr({'z' : z})\n",
    "        g.update_all(fn.copy_src(src='z', out='m'), fn.sum(msg='m', out='z'))\n",
    "        z_list.append(g.get_n_repr()['z'])\n",
    "        for i in range(self.radius - 1):\n",
    "            for j in range(2 ** i):\n",
    "                g.update_all(fn.copy_src(src='z', out='m'), fn.sum(msg='m', out='z'))\n",
    "            z_list.append(g.get_n_repr()['z'])\n",
    "        return z_list\n",
    "\n",
    "    def forward(self, g, lg, x, y, deg_g, deg_lg, pm_pd):\n",
    "        pmpd_x = F.embedding(pm_pd, x)\n",
    "\n",
    "        sum_x = sum(theta(z) for theta, z in zip(self.theta_list, self.aggregate(g, x)))\n",
    "        \n",
    "        g.set_e_repr({'y' : y})\n",
    "        g.update_all(fn.copy_edge(edge='y', out='m'), fn.sum('m', 'pmpd_y'))\n",
    "        pmpd_y = g.pop_n_repr('pmpd_y')\n",
    "\n",
    "        x = self.theta_x(x) + self.theta_deg(deg_g * x) + sum_x + self.theta_y(pmpd_y)\n",
    "        n = self.out_feats // 2\n",
    "        x = th.cat([x[:, :n], F.relu(x[:, n:])], 1)\n",
    "        x = self.bn_x(x)\n",
    "\n",
    "        sum_y = sum(gamma(z) for gamma, z in zip(self.gamma_list, self.aggregate(lg, y)))\n",
    "        y = self.gamma_y(y)\n",
    "        \n",
    "        y = th.cat([y[:, :n], F.relu(y[:, n:])], 1)\n",
    "        y = self.bn_y(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, feats, radius, n_classes):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : networkx.DiGraph\n",
    "        \"\"\"\n",
    "        super(GNN, self).__init__()\n",
    "        self.linear = nn.Linear(feats[-1], n_classes)\n",
    "        self.y_feats = [1] + feats[1:]\n",
    "        self.module_list = nn.ModuleList([GNNModule(m, m_y, n, radius)\n",
    "                                          for m, m_y, n in zip(feats[:-1],self.y_feats[:-1], feats[1:])])\n",
    "\n",
    "    def forward(self, g, lg, deg_g, deg_lg, pm_pd, feature):\n",
    "        x, y = feature, deg_lg\n",
    "        for module in self.module_list:\n",
    "            x, y = module(g, lg, x, y, deg_g, deg_lg, pm_pd)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(z_list, labels):\n",
    "    accu = []\n",
    "    ybar_list = [th.max(z, 1)[1] for z in z_list]\n",
    "    for y_bar in ybar_list:\n",
    "        accuracy = max(th.sum(y_bar == label).item() for label in labels) / len(labels[0])\n",
    "        accu.append(accuracy)\n",
    "    return sum(accu) / len(accu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def step(i, j, g, lg, deg_g, deg_lg, pm_pd, feature, label, equi_label, n_batchsize):\n",
    "    \"\"\" One step of training. \"\"\"\n",
    "    t0 = time.time()\n",
    "    z = model(g, lg, deg_g, deg_lg, pm_pd, feature)\n",
    "    time_forward = time.time() - t0\n",
    "\n",
    "    z_list = th.chunk(z, n_batchsize, 0)\n",
    "    equi_labels = [label, equi_label]\n",
    "    loss = sum(min(F.cross_entropy(z, y) for y in equi_labels) for z in z_list) / n_batchsize\n",
    "    \n",
    "    accu = accuracy(z_list, equi_labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    t0 = time.time()\n",
    "    loss.backward()\n",
    "    time_backward = time.time() - t0\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, accu, time_forward, time_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(g, lg, deg_g, deg_lg, pm_pd, feature, label, equi_label):\n",
    "    z = model(g, lg, deg_g, deg_lg, pm_pd, feature)\n",
    "    \n",
    "    z_list = [z]\n",
    "    \n",
    "    equi_labels = [label, equi_label]\n",
    "    \n",
    "    accu = accuracy(z_list, equi_labels)\n",
    "    \n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_viz(g, lg, deg_g, deg_lg, pm_pd, feature):\n",
    "    z = model(g, lg, deg_g, deg_lg, pm_pd, feature)\n",
    "    \n",
    "    z_list = [z]\n",
    "    \n",
    "    n_batchsize = 1\n",
    "    ybar_list = [th.max(z, 1)[1] for z in z_list]\n",
    "    cora_viz(ybar_list[0],g.to_networkx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "n_features = 16\n",
    "n_layers = 3\n",
    "radius = 2\n",
    "lr = 1e-2\n",
    "K = 2 # num_of_classes\n",
    "feats = [features.shape[1]] + [n_features]*n_layers + [K]\n",
    "model = GNN(feats, radius, K)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/huangqi/dgl2/dgl/python/dgl/graph.py:657: UserWarning: to_networkx currently does not support converting node/edge features automatically.\n",
      "  dgl_warning('to_networkx currently does not support converting'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train is 21\n",
      "the selection is [9]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "n_batch_size = 1\n",
    "split = 20\n",
    "\n",
    "training_dataset = CORABinary(g, features, labels, num_classes=7)\n",
    "num_train = len(training_dataset)\n",
    "print(\"num_train is\", num_train)\n",
    "indices = list(range(num_train))\n",
    "validation_idx = np.random.choice(indices, size=1)\n",
    "print(\"the selection is\", validation_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "training_loader = DataLoader(training_dataset, \n",
    "                             n_batch_size,\n",
    "                             collate_fn=training_dataset.collate_fn, \n",
    "                             drop_last=True)\n",
    "validation_loader = DataLoader(training_dataset,\n",
    "                     n_batch_size,\n",
    "                     collate_fn = training_dataset.collate_fn,\n",
    "                     sampler=validation_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "average loss for epoch 0 is 0.6013627052307129, with avg accu 0.6641352081342209, forward time 3.8709099292755127s, backward time 1.688732385635376 s\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n",
      "torch.FloatTensor torch.FloatTensor torch.LongTensor torch.FloatTensor torch.LongTensor torch.LongTensor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-6fd59b210951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                                                  \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                  \u001b[0mequivariant_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                                  n_batch_size)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ms_forward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-36ac5d351fb9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(i, j, g, lg, deg_g, deg_lg, pm_pd, feature, label, equi_label, n_batchsize)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtime_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/huangqi/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/huangqi/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#main loop\n",
    "\n",
    "\n",
    "\n",
    "n_iterations = 20\n",
    "n_epochs = 10\n",
    "validation_example_label_change = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    total_loss, total_accu, s_forward, s_backward = 0, 0, 0, 0\n",
    "    for j, [g, lg, g_deg, lg_deg, pm_pd, subfeature, label, equivariant_label] in enumerate(training_loader):\n",
    "        print(g_deg.type(), lg_deg.type(), pm_pd.type(), subfeature.type(), label.type(), equivariant_label.type())\n",
    "        loss, accu, t_forward, t_backward = step(i,\n",
    "                                                 j,\n",
    "                                                 g,\n",
    "                                                 lg,\n",
    "                                                 g_deg,\n",
    "                                                 lg_deg,\n",
    "                                                 pm_pd,\n",
    "                                                 subfeature,\n",
    "                                                 label,\n",
    "                                                 equivariant_label,\n",
    "                                                 n_batch_size)\n",
    "        total_loss += loss\n",
    "        s_forward += t_forward\n",
    "        s_backward += t_backward\n",
    "        total_accu += accu\n",
    "    \n",
    "    print(\"average loss for epoch {} is {}, with avg accu {}, forward time {}s, backward time {} s\".format(i, total_loss/len(training_loader), total_accu/len(training_loader), s_forward, s_backward))\n",
    "    [g, lg, g_deg, lg_deg, pm_pd, subfeature, label, equi_label] = next(iter(validation_loader))\n",
    "    \n",
    "    z = model(g, lg, g_deg, lg_deg, pm_pd, subfeature)\n",
    "    validation_example_label_change.append(th.max(z, 1)[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"inference on one training example...\")\n",
    "for k in range(len(training_loader) - split):\n",
    "    [g, lg, g_deg, lg_deg, pm_pd, subfeature, label, equi_label] = next(iter(validation_loader))\n",
    "    accu = val(g, \n",
    "               lg, \n",
    "               g_deg, \n",
    "               lg_deg, \n",
    "               pm_pd, \n",
    "               subfeature,\n",
    "               label,\n",
    "               equi_label)\n",
    "    print(\"#########\")\n",
    "    print(\"inference accu {}\".format(accu))\n",
    "    print(\"#########\")\n",
    "    inference_viz(g,\n",
    "                  lg,\n",
    "                  g_deg,\n",
    "                  lg_deg,\n",
    "                  pm_pd,\n",
    "                  subfeature)\n",
    "    plt.show()\n",
    "    cora_viz(label, g.to_networkx())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.ones(3)\n",
    "y = th.ones(4)\n",
    "th.cat([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cora_viz_ani(labels,subgraph):\n",
    "    value = [labels[node] for node in subgraph.nodes()]\n",
    "    pos = nx.spring_layout(subgraph, random_state=1)\n",
    "    nx.draw_networkx(subgraph, \n",
    "                     pos=pos, \n",
    "                     edge_color='k', \n",
    "                     node_size=5, \n",
    "                     cmap=plt.get_cmap('Set2'), \n",
    "                     node_color=value,\n",
    "                     with_labels=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(8,8), dpi=150)\n",
    "fig2.clf()\n",
    "ax = fig2.subplots()\n",
    "[g, lg, g_deg, lg_deg, pm_pd, subfeature, label, equi_label] = next(iter(validation_loader))\n",
    "g_nx = g.to_networkx()\n",
    "pos = nx.spring_layout(g_nx, random_state=1)\n",
    "\n",
    "def classify_animate(i):\n",
    "    ax.cla()\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"community detection result @ epoch %d\" % i)\n",
    "    value = [validation_example_label_change[i][node] for node in g_nx.nodes()]\n",
    "    nx.draw_networkx(g_nx,\n",
    "                     pos=pos,\n",
    "                     edge_color='k',\n",
    "                     node_size=7,\n",
    "                     cmap=plt.get_cmap('Set2'),\n",
    "                     node_color=value,\n",
    "                     arrows=False,\n",
    "                     width=0.6,\n",
    "                     with_labels=False)\n",
    "    \n",
    "\n",
    "ani = animation.FuncAnimation(fig2, classify_animate, frames=len(validation_example_label_change), interval=500)\n",
    "ani.save('./animation.gif', writer='imagemagick')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toygraph = DGLGraph()\n",
    "toygraph.add_nodes(4)\n",
    "toygraph.add_edges([0,1,1,0,3,3,2],[1,0,2,3,2,0,0])\n",
    "nx.draw(toygraph.to_networkx(), with_labels=True)\n",
    "lg = toygraph.line_graph(backtracking=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(lg.to_networkx(), with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
