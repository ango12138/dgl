{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GNN consists of two layers. The first layer embeds the attributes in various nodes. The second layer is a latent (hidden) representation of the nodes. No dropout is used currently. We should explore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "\n",
    "class HeteroRGCNLayer1(nn.Module):\n",
    "    EMBED_SIZE = 64\n",
    "    VOCAB_SIZE = 2386\n",
    "    def __init__(self, hidden_size, G):\n",
    "        super(HeteroRGCNLayer1, self).__init__()\n",
    "        # Need an embedding layer for each node feature\n",
    "        self.node_embeddings = {}\n",
    "        #self.dropouts = {}\n",
    "        for ntype in G.ntypes:\n",
    "            # create an embedding for each feature of a node\n",
    "            self.node_embeddings[ntype] = {}\n",
    "            num_node_features = G.node_attr_schemes(ntype)['f'].shape[0]\n",
    "            for feature in range(num_node_features):\n",
    "                self.node_embeddings[ntype][feature] = nn.Embedding(self.VOCAB_SIZE, self.EMBED_SIZE)\n",
    "            #self.dropouts[ntype] = nn.Dropout()\n",
    "        #for name in etypes:\n",
    "        module_layers = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            num_features = G.node_attr_schemes(srctype)['f'].shape[0]\n",
    "            module_layers[etype] = nn.Linear(num_features * self.EMBED_SIZE, hidden_size)\n",
    "        self.weight = nn.ModuleDict(module_layers)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, G):\n",
    "\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            # for each node compute the embedding and store it in the graph\n",
    "            # iterate over the features of each node and compute the embedding\n",
    "            the_node_embedding = self.node_embeddings[srctype]\n",
    "            node_feature_embeddings = []\n",
    "            num_features = G.node_attr_schemes(srctype)['f'].shape[0]\n",
    "            for feature in range(num_features):\n",
    "                feature_embedding_layer = the_node_embedding[feature]\n",
    "                node_feature_embeddings.append(feature_embedding_layer(G.nodes[srctype].data['f'][:, feature]))\n",
    "            comp_node_embedding = torch.cat(node_feature_embeddings, 1)\n",
    "            G.nodes[srctype].data['E'] = comp_node_embedding\n",
    "            # Compute W_r * h\n",
    "            Wh = self.weight[etype](G.nodes[srctype].data['E'])  \n",
    "            #Wh = torch.sum(Wh, dim = 1)\n",
    "                # Save it in graph for message passing\n",
    "            G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "            # Specify per-relation message passing functions: (message_func, reduce_func).\n",
    "            # Note that the results are saved to the same destination feature 'h', which\n",
    "            # hints the type wise reducer for aggregation.\n",
    "            funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        # Trigger message passing of multiple types.\n",
    "        # The first argument is the message passing functions for each relation.\n",
    "        # The second one is the type wise reducer, could be \"sum\", \"max\",\n",
    "        # \"min\", \"mean\", \"stack\"\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        #return G\n",
    "        return {ntype : G.nodes[ntype].data['h'] for ntype in G.ntypes}\n",
    "\n",
    "class HeteroRGCNLayer2(nn.Module):\n",
    "    def __init__(self, in_size, out_size, etypes):\n",
    "        super(HeteroRGCNLayer2, self).__init__()\n",
    "        # W_r for each relation\n",
    "        \n",
    "        self.weight = nn.ModuleDict({\n",
    "                name : nn.Linear(in_size, out_size) for name in etypes\n",
    "            })\n",
    "\n",
    "    def forward(self, G, feat_dict):\n",
    "        # The input is a dictionary of node features for each type\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            # Compute W_r * h\n",
    "            Wh = self.weight[etype](feat_dict[srctype])\n",
    "            # Save it in graph for message passing\n",
    "            G.nodes[srctype].data['Wh2_%s' % etype] = Wh\n",
    "            # Specify per-relation message passing functions: (message_func, reduce_func).\n",
    "            # Note that the results are saved to the same destination feature 'h', which\n",
    "            # hints the type wise reducer for aggregation.\n",
    "            funcs[etype] = (fn.copy_u('Wh2_%s' % etype, 'm'), fn.mean('m', 'h2'))\n",
    "        # Trigger message passing of multiple types.\n",
    "        # The first argument is the message passing functions for each relation.\n",
    "        # The second one is the type wise reducer, could be \"sum\", \"max\",\n",
    "        # \"min\", \"mean\", \"stack\"\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        #return G\n",
    "        # return the updated node feature dictionary\n",
    "        return {ntype : G.nodes[ntype].data['h2'] for ntype in G.ntypes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the graph\n",
    "The graph is loaded using the ITSM_Dataloader. This class:\n",
    "1. Creates the graph schema for the ITSM data in Arango DB\n",
    "2. Loads the data into Arango DB\n",
    "3. Creates a Graph in the format required by the DGL graph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test and train indices\n",
    "from ITSM_data_loader import ITSM_Dataloader\n",
    "\n",
    "dl = ITSM_Dataloader(create_db = False)\n",
    "labels, G = dl.get_networkx_graph(load_from_db=True, graph_desc_file_loc= \"graph_descriptor.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Graph Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.canonical_etypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the metagraph using graphviz.\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "nx.draw_networkx(G.metagraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.node_attr_schemes('incident')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Heterogeneous Graph Convolution Neural Network (HGCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroRGCN(nn.Module):\n",
    "    def __init__(self, G, hidden_size, out_size):\n",
    "        super(HeteroRGCN, self).__init__()\n",
    "        # create layers\n",
    "        self.layer1 = HeteroRGCNLayer1(hidden_size, G)\n",
    "        self.layer2 = HeteroRGCNLayer2(hidden_size, out_size, G.etypes)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, G):\n",
    "        \n",
    "        h_dict = self.layer1(G)\n",
    "        h_dict = {k : F.leaky_relu(h) for k, h in h_dict.items()}\n",
    "        h_dict = self.layer2(G, h_dict)\n",
    "        \n",
    "        # get paper logits\n",
    "        \n",
    "        return h_dict['incident']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mask = np.random.rand(len(labels)) <= 0.8\n",
    "train_idx = [i for i in range(len(labels)) if training_mask[i]]\n",
    "test_idx = [i for i in range(len(labels)) if not training_mask[i]]\n",
    "train_idx = torch.tensor(train_idx).long()\n",
    "test_idx = torch.tensor(test_idx).long()\n",
    "labels = torch.tensor(labels).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the HGCN and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create the model. The output has three logits for three classes.\n",
    "model = HeteroRGCN(G,32,2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    opt.zero_grad()\n",
    "    logits = model(G)\n",
    "    # The loss is computed only for labeled nodes.\n",
    "    loss = loss_fn(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    pred_trng = torch.argmax(logits[train_idx], dim = 1)\n",
    "    res_trng = pred_trng == labels[train_idx]\n",
    "    trng_acc = torch.sum(res_trng).item()/labels[train_idx].shape[0]\n",
    "    \n",
    "    pred_test = torch.argmax(logits[test_idx], dim = 1)\n",
    "    res_test = pred_test == labels[test_idx]\n",
    "    test_acc = torch.sum(res_test).item()/labels[test_idx].shape[0]\n",
    "    \n",
    "    \n",
    "   \n",
    "    if epoch % 10 == 0:\n",
    "        print( 'Loss %.4f, training accuracy %.4f, test accuracy %.4f' % ( loss.item(), trng_acc, test_acc ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
