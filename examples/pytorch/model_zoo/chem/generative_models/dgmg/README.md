# Learning Deep Generative Models of Graphs (DGMG)

Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. 
Learning Deep Generative Models of Graphs. *arXiv preprint arXiv:1803.03324*, 2018.

DGMG generates graphs by progressively adding nodes and edges as below:
![](https://user-images.githubusercontent.com/19576924/48605003-7f11e900-e9b6-11e8-8880-87362348e154.png)

For molecules, the nodes are atoms and the edges are bonds.

## Dataset

### ChEMBL

The authors use the [ChEMBL database](https://www.ebi.ac.uk/chembl/). Since they 
did not release the code, we use a subset from [Olivecrona et al.](https://github.com/MarcusOlivecrona/REINVENT), 
another work on generative modeling. 

The authors restrict their dataset to molecules with at most 20 heavy atoms, and used a training/validation
split of 130, 830/26, 166 examples each. We use the same split but need to relax 20 to 23 as we are using
a different subset.

### ZINC

After the pre-processing, we are left with 232464 molecules for training and 5000 molecules for validation.

## Usage

### Using new datasets

If you want to train with new datasets, you can proceed as follows:
1. Get the list of atom types and bond types from a list of molecule SMILES. 
You can use `get_atom_and_bond_types` at `L370, utils.py`.
2. Since DGMG has some limitation in generating molecules (E.g. it ignores things like protonation and chirality), we
need to standardize our dataset to avoid false novel molecules. You can preprocess your dataset with 
`preprocess_dataset` at `L638, utils.py`. 
3. Modify the dataset class `MoleculeDataset._setup` at `L753, utils.py`.

*Note1*: The first two steps have been performed before for our supported datasets so we do not re-perform them in
the training scripts.

*Note2*: With the data preprocessing, we've also removed molecules consisting of [N+], [O-] as DGMG cannot generate
them. For example, it can only generate `O=C1NC(=S)NC(=O)C1=CNC1=CC=C(N(=O)O)C=C1O` from 
`O=C1NC(=S)NC(=O)C1=CNC1=CC=C([N+](=O)[O-])C=C1O` even with correct decisions.

### Training

Training auto-regressive generative models tends to be very slow. According to the authors, they use multiprocess to
speed up training and gpu does not give much speed advantage. We follow their approach and perform multiprocess cpu
training.

To start training, do `python train.py -np X -d Y -o Z`, where:
- `X` is the number of processes to use. We use `32`. 
- `Y` is the dataset to use. We currently support `ChEMBL` and `ZINC`. 
- `Z` is the order for generating decision sequences, which can be either `canonical`, or `random`.

Even though multiprocess yields a significant speedup comparing to a single process, the training can still take a long 
time. An epoch of training and validation can take up to one hour and a half on our machine. If not necessary, we 
recommend users use our pre-trained models. 

Meanwhile, we make a checkpoint of our model whenever there is a performance improvement on the validation set so you 
do not need to wait until the training terminates.

#### Monitoring

We can monitor the training process with tensorboard as below:

![](https://s3.us-east-2.amazonaws.com/dgl.ai/model_zoo/drug_discovery/dgmg/tensorboard.png)

To use tensorboard, you need to install [tensorboardX](https://github.com/lanpa/tensorboardX) and 
[TensorFlow](https://www.tensorflow.org/). You can lunch tensorboard with `tensorboard --logdir=.`

### Evaluation

To evaluate your own model, do `python eval.py -d X -o Y -p Z -np N`, where:
- `X` is the dataset to use, such as `ChEMBL` and `ZINC`
- `Y` is the order for generating decision sequences, which can be either `canonical`, or `random`. 
For evaluation, this is only used to name directory.
- `Z` is the path to model. By default, this should be a `checkpoint.pth` under directory `training_results`.
- `N` is the number of processes to use. We use `64`.

To use a pretrained model, replace `-p Z` with `-pr`.

After the evaluation, 100000 molecules will be generated and stored in `generated_smiles.txt` under `eval_results`
directory, with three statistics logged in `generation_stats.txt` under `eval_results`:
1. `Validity among all` gives the percentage of molecules that are valid
2. `Uniqueness among valid ones` gives the percentage of valid molecules that are unique
3. `Novelty among unique ones` gives the percentage of unique valid molecules that are novel (not seen in training data)

### Pre-trained models

Below are example molecules generated by our pretrained model `ChEMBL_canonical`.

![](https://s3.us-east-2.amazonaws.com/dgl.ai/model_zoo/DGMG_ChEMBL_canonical.png)

Below gives the statistics of pre-trained models. With random order, the training becomes significantly more difficult 
as we now have `N^2` data points with `N` molecules.

| Pre-trained model | % valid | % unique among valid | % novel among unique |
| ----------------- | ------- | -------------------- | -------------------- |
| `ChEMBL_canonical`  | 78.18   | 99.13                | 98.64                |            
| `ChEMBL_random`     | 27.19   | 99.73                | 100.00               |
| `ZINC_canonical`    | 74.20   | 99.86                | 99.90                |
| `ZINC_random`       | 17.85   | 99.34                | 100.00               |
