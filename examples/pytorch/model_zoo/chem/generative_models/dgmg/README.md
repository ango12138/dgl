# Learning Deep Generative Models of Graphs (DGMG)

Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. 
Learning Deep Generative Models of Graphs. *arXiv preprint arXiv:1803.03324*, 2018.

DGMG generates graphs by progressively adding nodes and edges as below:
![](https://user-images.githubusercontent.com/19576924/48605003-7f11e900-e9b6-11e8-8880-87362348e154.png)

For molecules, the nodes are atoms and the edges are bonds.

## Dataset

### Preprocessing

With our implementation, this model has several limitations:
1. Information about protonation and chirality are ignored during generation
2. Molecules consisting of `[N+]`, `[O-]`, etc. cannot be generated.

For example, the model can only generate `O=C1NC(=S)NC(=O)C1=CNC1=CC=C(N(=O)O)C=C1O` from 
`O=C1NC(=S)NC(=O)C1=CNC1=CC=C([N+](=O)[O-])C=C1O` even with the correct decisions.

To avoid issues about validity and novelty, we filter out these molecules from the dataset.

### ChEMBL

The authors use the [ChEMBL database](https://www.ebi.ac.uk/chembl/). Since they 
did not release the code, we use a subset from [Olivecrona et al.](https://github.com/MarcusOlivecrona/REINVENT), 
another work on generative modeling. 

The authors restrict their dataset to molecules with at most 20 heavy atoms, and used a training/validation
split of 130, 830/26, 166 examples each. We use the same split but need to relax 20 to 23 as we are using
a different subset.

### ZINC

After the pre-processing, we are left with 232464 molecules for training and 5000 molecules for validation.

## Usage

### Training

Training auto-regressive generative models tends to be very slow. According to the authors, they use multiprocess to
speed up training and gpu does not give much speed advantage. We follow their approach and perform multiprocess cpu
training.

To start training, do `python train.py -np X -d Y -o Z`, where:
- `X` is the number of processes to use. We use `32`. 
- `Y` is the dataset to use. We currently support `ChEMBL` and `ZINC`. 
- `Z` is the order for generating decision sequences, which can be either `canonical`, or `random`.

Even though multiprocess yields a significant speedup comparing to a single process, the training can still take a long 
time. An epoch of training and validation can take up to one hour and a half on our machine. If not necessary, we 
recommend users use our pre-trained models. 

Meanwhile, we make a checkpoint of our model whenever there is a performance improvement on the validation set so you 
do not need to wait until the training terminates.

#### Dataset configuration

You can also use your own dataset with `python train.py -np X -d Y -o Z -tf A -vf B`, where:
- `A` is the path to a file for the training data with one SMILES a line
- `B` is the path to a file for the validation data with one SMILES a line

#### Monitoring

We can monitor the training process with tensorboard as below:

![](https://s3.us-east-2.amazonaws.com/dgl.ai/model_zoo/drug_discovery/dgmg/tensorboard.png)

To use tensorboard, you need to install [tensorboardX](https://github.com/lanpa/tensorboardX) and 
[TensorFlow](https://www.tensorflow.org/). You can lunch tensorboard with `tensorboard --logdir=.`

### Evaluation

To evaluate your own model, do `python eval.py -d X -o Y -p Z -np A -mn B -gt C`, where:
- `X` is the dataset to use, such as `ChEMBL` and `ZINC`
- `Y` is the order for generating decision sequences, which can be either `canonical`, or `random`. 
For evaluation, this is only used to name directory.
- `Z` is the path to model. By default, this should be a `checkpoint.pth` under directory `training_results`.
- `A` is the number of processes to use. We use `32`.
- `B` is the maximum number of atoms allowed in a molecule to prevent the model from not stopping. 
By default we use `25`.
- `C` is the max time (in seconds) allowed for molecule generation. By default we use `600` (10 minutes).

To use a pretrained model, replace `-p Z` with `-pr`.

After the evaluation, 100000 molecules will be generated and stored in `generated_smiles.txt` under `eval_results`
directory, with three statistics logged in `generation_stats.txt` under `eval_results`:
1. `Validity among all` gives the percentage of molecules that are valid
2. `Uniqueness among valid ones` gives the percentage of valid molecules that are unique
3. `Novelty among unique ones` gives the percentage of unique valid molecules that are novel (not seen in training data)

### Pre-trained models

Below are example molecules generated by our pretrained model `ChEMBL_canonical`.

![](https://s3.us-east-2.amazonaws.com/dgl.ai/model_zoo/DGMG_ChEMBL_canonical.png)

Below gives the statistics of pre-trained models. With random order, the training becomes significantly more difficult 
as we now have `N^2` data points with `N` molecules.

| Pre-trained model  | % valid | % unique among valid | % novel among unique |
| ------------------ | ------- | -------------------- | -------------------- |
| `ChEMBL_canonical` | 78.18   | 99.13              | 98.64                |            
| `ChEMBL_random`    | 28.75   | 99.80              | 100.00               |
| `ZINC_canonical`   | 78.12   | 99.84              | 99.86                |
| `ZINC_random`      | 24.64   | 99.70              | 100.00               |
