{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DGL_HOME'] = os.getcwd()\n",
    "os.environ['PYTHONPATH'] = '{}/python:{}'.format(os.environ['DGL_HOME'], os.environ['PYTHONPATH'])\n",
    "os.environ['DGL_LIBRARY_PATH'] = '{}/build'.format(os.environ['DGL_HOME'])\n",
    "os.environ['DGLBACKEND'] = 'mxnet'\n",
    "#os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'\n",
    "sys.path.append('{}/python'.format(os.environ['DGL_HOME']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--normalization'], dest='normalization', nargs=None, const=None, default=None, type=None, choices=['sym', 'left'], help='graph normalization types (default=None)', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='GCMC')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.2,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=-1,\n",
    "        help=\"gpu\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.01,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument(\"--wd\", type=float, default=1e-4,\n",
    "        help=\"weight decay\")\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=200,\n",
    "        help=\"number of training epochs\")\n",
    "parser.add_argument(\"--n-hidden\", type=int, default=6,\n",
    "        help=\"number of hidden gcn units\")\n",
    "parser.add_argument(\"--n-layers\", type=int, default=1,\n",
    "        help=\"number of hidden gcn layers\")\n",
    "parser.add_argument(\"--normalization\",\n",
    "        choices=['sym','left'], default=None,\n",
    "        help=\"graph normalization types (default=None)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dropout=0.2, gpu=-1, lr=0.01, n_epochs=200, n_hidden=6, n_layers=1, normalization='left', wd=0.0001)\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(['--normalization','left',])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data.utils import download, get_download_dir, extract_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_urls = {\n",
    "    'ml-100k' : 'http://files.grouplens.org/datasets/movielens/ml-100k.zip',\n",
    "    'ml-1m'   : 'http://files.grouplens.org/datasets/movielens/ml-1m.zip',\n",
    "#     'ml-10m'  : 'http://files.grouplens.org/datasets/movielens/ml-10m.zip',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = {\n",
    "    'ml-100k':'ml-100k/u.data',\n",
    "    'ml-1m':'ml-1m/ratings.dat',\n",
    "#     'ml-10m':'ml-10M100K/ratings.dat',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sep = lambda k:{\n",
    "    'ml-1m'  : '::',\n",
    "#     'ml-10m' : '::',\n",
    "}.get(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml-100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/yifeim-work/dgl/python/dgl/data/../../../_download/ml-100k.zip'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(_urls[dataset],\n",
    "         '{}/{}.zip'.format(get_download_dir(), dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = ZipFile('{}/{}.zip'.format(get_download_dir(), dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ml-100k/',\n",
       " 'ml-100k/allbut.pl',\n",
       " 'ml-100k/mku.sh',\n",
       " 'ml-100k/README',\n",
       " 'ml-100k/u.data',\n",
       " 'ml-100k/u.genre',\n",
       " 'ml-100k/u.info',\n",
       " 'ml-100k/u.item',\n",
       " 'ml-100k/u.occupation',\n",
       " 'ml-100k/u.user',\n",
       " 'ml-100k/u1.base',\n",
       " 'ml-100k/u1.test',\n",
       " 'ml-100k/u2.base',\n",
       " 'ml-100k/u2.test',\n",
       " 'ml-100k/u3.base',\n",
       " 'ml-100k/u3.test',\n",
       " 'ml-100k/u4.base',\n",
       " 'ml-100k/u4.test',\n",
       " 'ml-100k/u5.base',\n",
       " 'ml-100k/u5.test',\n",
       " 'ml-100k/ua.base',\n",
       " 'ml-100k/ua.test',\n",
       " 'ml-100k/ub.base',\n",
       " 'ml-100k/ub.test']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zf.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1, '2': 2, '3': 3, '4': 4, '5': 5}\n"
     ]
    }
   ],
   "source": [
    "users = {}\n",
    "items = {}\n",
    "ratings = {}\n",
    "with zf.open(_path[dataset]) as f:\n",
    "    for line in f:\n",
    "        user, item, rating, _ = line.decode('latin1').strip('\\n').split(get_sep(dataset))\n",
    "        \n",
    "        users.setdefault(user, 0)\n",
    "        users[user] += 1\n",
    "        \n",
    "        items.setdefault(item, 0)\n",
    "        items[item] += 1\n",
    "        \n",
    "        ratings.setdefault(rating, 0)\n",
    "        ratings[rating] += 1\n",
    "\n",
    "user_idx = {k:i\n",
    "            for i,k in enumerate(sorted(users))}\n",
    "\n",
    "item_idx = {k:i+len(users)\n",
    "            for i,k in enumerate(sorted(items))}\n",
    "\n",
    "rating_idx = {k:i+1 for i,k in\n",
    "              enumerate(sorted(ratings.keys()))}\n",
    "\n",
    "print(rating_idx)\n",
    "n_classes = max(rating_idx.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl import DGLGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx=mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = DGLGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes(len(user_idx) + len(item_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.ndata['is_user'] = mx.nd.concat(\n",
    "    mx.nd.ones(len(user_idx), ctx=ctx),\n",
    "    mx.nd.zeros(len(item_idx), ctx=ctx),\n",
    "    dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = []\n",
    "v = []\n",
    "r = []\n",
    "with zf.open(_path[dataset]) as f:\n",
    "    for line in f:\n",
    "        user, item, rating, _ = line.decode('latin1').strip('\\n').split(get_sep(dataset))\n",
    "        u.append(user_idx[user])\n",
    "        v.append(item_idx[item])\n",
    "        r.append(rating_idx[rating])\n",
    "\n",
    "G.add_edges(u, v, {\n",
    "    'r': mx.nd.array(r, dtype='float32', ctx=ctx),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edata['_rand'] = mx.nd.random.uniform_like(G.edata['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edata['is_train'] = G.edata['_rand'] <= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edata['is_test'] = G.edata['_rand'] > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edges(*reversed(G.all_edges()), data=G.edata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': \n",
       "[ 3.  3.  1. ...,  1.  2.  3.]\n",
       "<NDArray 200000 @gpu(0)>, '_rand': \n",
       "[ 0.66865093  0.17409194  0.38500249 ...,  0.09558475  0.52363914\n",
       "  0.3631283 ]\n",
       "<NDArray 200000 @gpu(0)>, 'is_train': \n",
       "[ 1.  1.  1. ...,  1.  1.  1.]\n",
       "<NDArray 200000 @gpu(0)>, 'is_test': \n",
       "[ 0.  0.  0. ...,  0.  0.  0.]\n",
       "<NDArray 200000 @gpu(0)>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.edata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# degree and average ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/yifeim-work/dgl/python/dgl/frame.py:204: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  dgl_warning('Initializer is not set. Use zero initializer instead.'\n"
     ]
    }
   ],
   "source": [
    "G.update_all(\n",
    "    lambda edge : {\n",
    "        # n_edges, n_classes\n",
    "        'm': mx.nd.one_hot(edge.data['r'], n_classes) *\n",
    "             edge.data['is_train'].expand_dims(1)\n",
    "    },\n",
    "    lambda node : {\n",
    "        # n_nodes, n_edges, n_classes\n",
    "        'degree' : mx.nd.sum(node.mailbox['m'], axis=1)\n",
    "        # n_nodes, n_classes\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_user': \n",
       "[ 1.  1.  1. ...,  0.  0.  0.]\n",
       "<NDArray 2625 @gpu(0)>, 'degree': \n",
       "[[  0.  19.  22.  43.  67.  64.]\n",
       " [  0.   0.   0.  13.  93.  42.]\n",
       " [  0.   4.   7.  18.  15.   2.]\n",
       " ..., \n",
       " [  0.   6.   2.   5.   2.   0.]\n",
       " [  0.   3.   3.   2.   1.   2.]\n",
       " [  0.   1.   3.   1.   3.   0.]]\n",
       "<NDArray 2625x6 @gpu(0)>}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_naive(edge):\n",
    "    pred = (mx.nd.dot(edge.src['h'], mx.nd.arange(n_classes, ctx=ctx))+\n",
    "            mx.nd.dot(edge.dst['h'], mx.nd.arange(n_classes, ctx=ctx)))/2.\n",
    "    return {'pred' : pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/yifeim-work/dgl/python/dgl/frame.py:204: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  dgl_warning('Initializer is not set. Use zero initializer instead.'\n"
     ]
    }
   ],
   "source": [
    "G.ndata['h'] = G.ndata['degree'] / (\n",
    "    G.ndata['degree'].sum(axis=1, keepdims=True) + 1e-10)\n",
    "G.apply_edges(quad_naive)\n",
    "del G.ndata['h']\n",
    "avg_pred = G.edata.pop('pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error 0.96376991272\n"
     ]
    }
   ],
   "source": [
    "print('training error', (2. * l2_loss(\n",
    "    avg_pred,\n",
    "    G.edata['r'],\n",
    "    G.edata['is_train']\n",
    ").sum() / (G.edata['is_train']).sum()).sqrt().asscalar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing error 0.984938442707\n"
     ]
    }
   ],
   "source": [
    "print('testing error', (2. * l2_loss(\n",
    "    avg_pred,\n",
    "    G.edata['r'],\n",
    "    G.edata['is_test']\n",
    ").sum() / (G.edata['is_test']).sum()).sqrt().asscalar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn quad_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from functools import partial\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadDense(gluon.Block):\n",
    "    def __init__(self):\n",
    "        super(QuadDense, self).__init__()\n",
    "        self.dense = gluon.nn.Dense(1, flatten=True)\n",
    "        \n",
    "    def forward(self, edge):\n",
    "        feat = (edge.src['h'] + edge.dst['h']) / 2.\n",
    "        pred = self.dense(feat)\n",
    "        return {'pred':pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_dense = QuadDense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_dense.collect_params().initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 6.7491 | Time(s) 0.0608 | ETputs(KTEPS) 3291.66\n",
      "Epoch 00001 | Loss 6.0696 | Time(s) 0.1050 | ETputs(KTEPS) 1905.32\n",
      "Epoch 00002 | Loss 5.4313 | Time(s) 0.1193 | ETputs(KTEPS) 1676.26\n",
      "Epoch 00003 | Loss 4.8348 | Time(s) 0.1263 | ETputs(KTEPS) 1583.23\n",
      "Epoch 00004 | Loss 4.2806 | Time(s) 0.1306 | ETputs(KTEPS) 1531.84\n",
      "Epoch 00005 | Loss 3.7691 | Time(s) 0.1334 | ETputs(KTEPS) 1498.83\n",
      "Epoch 00006 | Loss 3.3002 | Time(s) 0.1354 | ETputs(KTEPS) 1477.05\n",
      "Epoch 00007 | Loss 2.8739 | Time(s) 0.1369 | ETputs(KTEPS) 1460.71\n",
      "Epoch 00008 | Loss 2.4898 | Time(s) 0.1382 | ETputs(KTEPS) 1447.68\n",
      "Epoch 00009 | Loss 2.1471 | Time(s) 0.1391 | ETputs(KTEPS) 1437.89\n",
      "Epoch 00010 | Loss 1.8450 | Time(s) 0.1399 | ETputs(KTEPS) 1429.99\n",
      "Epoch 00011 | Loss 1.5821 | Time(s) 0.1406 | ETputs(KTEPS) 1422.75\n",
      "Epoch 00012 | Loss 1.3570 | Time(s) 0.1411 | ETputs(KTEPS) 1417.57\n",
      "Epoch 00013 | Loss 1.1676 | Time(s) 0.1416 | ETputs(KTEPS) 1412.32\n",
      "Epoch 00014 | Loss 1.0118 | Time(s) 0.1420 | ETputs(KTEPS) 1408.32\n",
      "Epoch 00015 | Loss 0.8871 | Time(s) 0.1424 | ETputs(KTEPS) 1404.85\n",
      "Epoch 00016 | Loss 0.7907 | Time(s) 0.1427 | ETputs(KTEPS) 1401.89\n",
      "Epoch 00017 | Loss 0.7197 | Time(s) 0.1429 | ETputs(KTEPS) 1399.42\n",
      "Epoch 00018 | Loss 0.6708 | Time(s) 0.1432 | ETputs(KTEPS) 1397.04\n",
      "Epoch 00019 | Loss 0.6408 | Time(s) 0.1434 | ETputs(KTEPS) 1394.78\n",
      "Epoch 00020 | Loss 0.6264 | Time(s) 0.1436 | ETputs(KTEPS) 1392.85\n",
      "Epoch 00021 | Loss 0.6243 | Time(s) 0.1438 | ETputs(KTEPS) 1391.20\n",
      "Epoch 00022 | Loss 0.6316 | Time(s) 0.1438 | ETputs(KTEPS) 1390.51\n",
      "Epoch 00023 | Loss 0.6453 | Time(s) 0.1436 | ETputs(KTEPS) 1393.12\n",
      "Epoch 00024 | Loss 0.6628 | Time(s) 0.1435 | ETputs(KTEPS) 1394.15\n",
      "Epoch 00025 | Loss 0.6817 | Time(s) 0.1434 | ETputs(KTEPS) 1395.09\n",
      "Epoch 00026 | Loss 0.7003 | Time(s) 0.1433 | ETputs(KTEPS) 1395.97\n",
      "Epoch 00027 | Loss 0.7169 | Time(s) 0.1432 | ETputs(KTEPS) 1396.85\n",
      "Epoch 00028 | Loss 0.7304 | Time(s) 0.1431 | ETputs(KTEPS) 1397.61\n",
      "Epoch 00029 | Loss 0.7401 | Time(s) 0.1430 | ETputs(KTEPS) 1398.36\n",
      "Epoch 00030 | Loss 0.7456 | Time(s) 0.1430 | ETputs(KTEPS) 1399.03\n",
      "Epoch 00031 | Loss 0.7467 | Time(s) 0.1429 | ETputs(KTEPS) 1399.67\n",
      "Epoch 00032 | Loss 0.7437 | Time(s) 0.1428 | ETputs(KTEPS) 1400.27\n",
      "Epoch 00033 | Loss 0.7368 | Time(s) 0.1428 | ETputs(KTEPS) 1400.77\n",
      "Epoch 00034 | Loss 0.7267 | Time(s) 0.1427 | ETputs(KTEPS) 1401.61\n",
      "Epoch 00035 | Loss 0.7138 | Time(s) 0.1426 | ETputs(KTEPS) 1402.14\n",
      "Epoch 00036 | Loss 0.6990 | Time(s) 0.1426 | ETputs(KTEPS) 1402.61\n",
      "Epoch 00037 | Loss 0.6830 | Time(s) 0.1425 | ETputs(KTEPS) 1403.10\n",
      "Epoch 00038 | Loss 0.6663 | Time(s) 0.1425 | ETputs(KTEPS) 1403.59\n",
      "Epoch 00039 | Loss 0.6497 | Time(s) 0.1425 | ETputs(KTEPS) 1403.96\n",
      "Epoch 00040 | Loss 0.6336 | Time(s) 0.1424 | ETputs(KTEPS) 1404.40\n",
      "Epoch 00041 | Loss 0.6185 | Time(s) 0.1424 | ETputs(KTEPS) 1404.80\n",
      "Epoch 00042 | Loss 0.6048 | Time(s) 0.1423 | ETputs(KTEPS) 1405.05\n",
      "Epoch 00043 | Loss 0.5926 | Time(s) 0.1423 | ETputs(KTEPS) 1405.48\n",
      "Epoch 00044 | Loss 0.5821 | Time(s) 0.1423 | ETputs(KTEPS) 1405.79\n",
      "Epoch 00045 | Loss 0.5733 | Time(s) 0.1414 | ETputs(KTEPS) 1414.08\n",
      "Epoch 00046 | Loss 0.5662 | Time(s) 0.1397 | ETputs(KTEPS) 1431.55\n",
      "Epoch 00047 | Loss 0.5607 | Time(s) 0.1382 | ETputs(KTEPS) 1447.48\n",
      "Epoch 00048 | Loss 0.5565 | Time(s) 0.1367 | ETputs(KTEPS) 1463.12\n",
      "Epoch 00049 | Loss 0.5534 | Time(s) 0.1352 | ETputs(KTEPS) 1478.93\n",
      "Epoch 00050 | Loss 0.5513 | Time(s) 0.1339 | ETputs(KTEPS) 1494.20\n",
      "Epoch 00051 | Loss 0.5499 | Time(s) 0.1326 | ETputs(KTEPS) 1508.65\n",
      "Epoch 00052 | Loss 0.5489 | Time(s) 0.1313 | ETputs(KTEPS) 1523.10\n",
      "Epoch 00053 | Loss 0.5481 | Time(s) 0.1301 | ETputs(KTEPS) 1537.39\n",
      "Epoch 00054 | Loss 0.5474 | Time(s) 0.1289 | ETputs(KTEPS) 1551.40\n",
      "Epoch 00055 | Loss 0.5466 | Time(s) 0.1278 | ETputs(KTEPS) 1565.02\n",
      "Epoch 00056 | Loss 0.5455 | Time(s) 0.1267 | ETputs(KTEPS) 1578.40\n",
      "Epoch 00057 | Loss 0.5442 | Time(s) 0.1256 | ETputs(KTEPS) 1591.89\n",
      "Epoch 00058 | Loss 0.5426 | Time(s) 0.1246 | ETputs(KTEPS) 1604.80\n",
      "Epoch 00059 | Loss 0.5406 | Time(s) 0.1236 | ETputs(KTEPS) 1617.51\n",
      "Epoch 00060 | Loss 0.5383 | Time(s) 0.1227 | ETputs(KTEPS) 1630.15\n",
      "Epoch 00061 | Loss 0.5357 | Time(s) 0.1217 | ETputs(KTEPS) 1642.75\n",
      "Epoch 00062 | Loss 0.5330 | Time(s) 0.1209 | ETputs(KTEPS) 1654.94\n",
      "Epoch 00063 | Loss 0.5300 | Time(s) 0.1200 | ETputs(KTEPS) 1666.79\n",
      "Epoch 00064 | Loss 0.5270 | Time(s) 0.1192 | ETputs(KTEPS) 1678.25\n",
      "Epoch 00065 | Loss 0.5240 | Time(s) 0.1184 | ETputs(KTEPS) 1689.71\n",
      "Epoch 00066 | Loss 0.5210 | Time(s) 0.1176 | ETputs(KTEPS) 1701.01\n",
      "Epoch 00067 | Loss 0.5181 | Time(s) 0.1168 | ETputs(KTEPS) 1712.84\n",
      "Epoch 00068 | Loss 0.5153 | Time(s) 0.1160 | ETputs(KTEPS) 1724.38\n",
      "Epoch 00069 | Loss 0.5127 | Time(s) 0.1152 | ETputs(KTEPS) 1735.76\n",
      "Epoch 00070 | Loss 0.5103 | Time(s) 0.1145 | ETputs(KTEPS) 1746.81\n",
      "Epoch 00071 | Loss 0.5081 | Time(s) 0.1138 | ETputs(KTEPS) 1757.18\n",
      "Epoch 00072 | Loss 0.5060 | Time(s) 0.1132 | ETputs(KTEPS) 1767.40\n",
      "Epoch 00073 | Loss 0.5041 | Time(s) 0.1125 | ETputs(KTEPS) 1777.47\n",
      "Epoch 00074 | Loss 0.5023 | Time(s) 0.1119 | ETputs(KTEPS) 1787.50\n",
      "Epoch 00075 | Loss 0.5006 | Time(s) 0.1113 | ETputs(KTEPS) 1797.34\n",
      "Epoch 00076 | Loss 0.4991 | Time(s) 0.1107 | ETputs(KTEPS) 1806.49\n",
      "Epoch 00077 | Loss 0.4976 | Time(s) 0.1101 | ETputs(KTEPS) 1815.94\n",
      "Epoch 00078 | Loss 0.4961 | Time(s) 0.1096 | ETputs(KTEPS) 1825.44\n",
      "Epoch 00079 | Loss 0.4947 | Time(s) 0.1090 | ETputs(KTEPS) 1834.42\n",
      "Epoch 00080 | Loss 0.4933 | Time(s) 0.1085 | ETputs(KTEPS) 1843.34\n",
      "Epoch 00081 | Loss 0.4918 | Time(s) 0.1080 | ETputs(KTEPS) 1852.09\n",
      "Epoch 00082 | Loss 0.4904 | Time(s) 0.1075 | ETputs(KTEPS) 1859.95\n",
      "Epoch 00083 | Loss 0.4890 | Time(s) 0.1070 | ETputs(KTEPS) 1868.36\n",
      "Epoch 00084 | Loss 0.4875 | Time(s) 0.1066 | ETputs(KTEPS) 1876.58\n",
      "Epoch 00085 | Loss 0.4861 | Time(s) 0.1061 | ETputs(KTEPS) 1884.62\n",
      "Epoch 00086 | Loss 0.4846 | Time(s) 0.1057 | ETputs(KTEPS) 1892.42\n",
      "Epoch 00087 | Loss 0.4832 | Time(s) 0.1053 | ETputs(KTEPS) 1900.03\n",
      "Epoch 00088 | Loss 0.4818 | Time(s) 0.1048 | ETputs(KTEPS) 1907.65\n",
      "Epoch 00089 | Loss 0.4804 | Time(s) 0.1044 | ETputs(KTEPS) 1914.87\n",
      "Epoch 00090 | Loss 0.4790 | Time(s) 0.1040 | ETputs(KTEPS) 1922.29\n",
      "Epoch 00091 | Loss 0.4777 | Time(s) 0.1036 | ETputs(KTEPS) 1930.06\n",
      "Epoch 00092 | Loss 0.4764 | Time(s) 0.1032 | ETputs(KTEPS) 1937.47\n",
      "Epoch 00093 | Loss 0.4751 | Time(s) 0.1028 | ETputs(KTEPS) 1944.85\n",
      "Epoch 00094 | Loss 0.4739 | Time(s) 0.1025 | ETputs(KTEPS) 1951.42\n",
      "Epoch 00095 | Loss 0.4727 | Time(s) 0.1021 | ETputs(KTEPS) 1958.32\n",
      "Epoch 00096 | Loss 0.4716 | Time(s) 0.1018 | ETputs(KTEPS) 1964.78\n",
      "Epoch 00097 | Loss 0.4705 | Time(s) 0.1014 | ETputs(KTEPS) 1971.58\n",
      "Epoch 00098 | Loss 0.4694 | Time(s) 0.1011 | ETputs(KTEPS) 1978.13\n",
      "Epoch 00099 | Loss 0.4684 | Time(s) 0.1008 | ETputs(KTEPS) 1984.64\n",
      "Epoch 00100 | Loss 0.4674 | Time(s) 0.1005 | ETputs(KTEPS) 1990.85\n",
      "Epoch 00101 | Loss 0.4664 | Time(s) 0.1002 | ETputs(KTEPS) 1996.96\n",
      "Epoch 00102 | Loss 0.4654 | Time(s) 0.0999 | ETputs(KTEPS) 2002.91\n",
      "Epoch 00103 | Loss 0.4645 | Time(s) 0.0996 | ETputs(KTEPS) 2008.67\n",
      "Epoch 00104 | Loss 0.4636 | Time(s) 0.0993 | ETputs(KTEPS) 2015.09\n",
      "Epoch 00105 | Loss 0.4627 | Time(s) 0.0990 | ETputs(KTEPS) 2020.80\n",
      "Epoch 00106 | Loss 0.4618 | Time(s) 0.0987 | ETputs(KTEPS) 2026.53\n",
      "Epoch 00107 | Loss 0.4609 | Time(s) 0.0984 | ETputs(KTEPS) 2031.84\n",
      "Epoch 00108 | Loss 0.4600 | Time(s) 0.0982 | ETputs(KTEPS) 2037.39\n",
      "Epoch 00109 | Loss 0.4592 | Time(s) 0.0979 | ETputs(KTEPS) 2043.06\n",
      "Epoch 00110 | Loss 0.4584 | Time(s) 0.0976 | ETputs(KTEPS) 2048.87\n",
      "Epoch 00111 | Loss 0.4576 | Time(s) 0.0974 | ETputs(KTEPS) 2054.27\n",
      "Epoch 00112 | Loss 0.4568 | Time(s) 0.0971 | ETputs(KTEPS) 2059.61\n",
      "Epoch 00113 | Loss 0.4561 | Time(s) 0.0968 | ETputs(KTEPS) 2065.23\n",
      "Epoch 00114 | Loss 0.4553 | Time(s) 0.0966 | ETputs(KTEPS) 2070.35\n",
      "Epoch 00115 | Loss 0.4546 | Time(s) 0.0964 | ETputs(KTEPS) 2075.46\n",
      "Epoch 00116 | Loss 0.4539 | Time(s) 0.0961 | ETputs(KTEPS) 2080.59\n",
      "Epoch 00117 | Loss 0.4532 | Time(s) 0.0959 | ETputs(KTEPS) 2085.46\n",
      "Epoch 00118 | Loss 0.4526 | Time(s) 0.0957 | ETputs(KTEPS) 2090.53\n",
      "Epoch 00119 | Loss 0.4519 | Time(s) 0.0955 | ETputs(KTEPS) 2095.27\n",
      "Epoch 00120 | Loss 0.4513 | Time(s) 0.0952 | ETputs(KTEPS) 2100.56\n",
      "Epoch 00121 | Loss 0.4507 | Time(s) 0.0950 | ETputs(KTEPS) 2105.51\n",
      "Epoch 00122 | Loss 0.4501 | Time(s) 0.0948 | ETputs(KTEPS) 2110.61\n",
      "Epoch 00123 | Loss 0.4495 | Time(s) 0.0946 | ETputs(KTEPS) 2115.14\n",
      "Epoch 00124 | Loss 0.4490 | Time(s) 0.0943 | ETputs(KTEPS) 2119.96\n",
      "Epoch 00125 | Loss 0.4484 | Time(s) 0.0941 | ETputs(KTEPS) 2124.71\n",
      "Epoch 00126 | Loss 0.4479 | Time(s) 0.0939 | ETputs(KTEPS) 2129.32\n",
      "Epoch 00127 | Loss 0.4474 | Time(s) 0.0937 | ETputs(KTEPS) 2133.72\n",
      "Epoch 00128 | Loss 0.4469 | Time(s) 0.0935 | ETputs(KTEPS) 2138.38\n",
      "Epoch 00129 | Loss 0.4464 | Time(s) 0.0933 | ETputs(KTEPS) 2142.74\n",
      "Epoch 00130 | Loss 0.4459 | Time(s) 0.0931 | ETputs(KTEPS) 2147.32\n",
      "Epoch 00131 | Loss 0.4454 | Time(s) 0.0929 | ETputs(KTEPS) 2152.18\n",
      "Epoch 00132 | Loss 0.4450 | Time(s) 0.0927 | ETputs(KTEPS) 2156.47\n",
      "Epoch 00133 | Loss 0.4445 | Time(s) 0.0926 | ETputs(KTEPS) 2160.56\n",
      "Epoch 00134 | Loss 0.4441 | Time(s) 0.0924 | ETputs(KTEPS) 2164.79\n",
      "Epoch 00135 | Loss 0.4437 | Time(s) 0.0922 | ETputs(KTEPS) 2169.17\n",
      "Epoch 00136 | Loss 0.4433 | Time(s) 0.0920 | ETputs(KTEPS) 2173.65\n",
      "Epoch 00137 | Loss 0.4429 | Time(s) 0.0918 | ETputs(KTEPS) 2178.07\n",
      "Epoch 00138 | Loss 0.4425 | Time(s) 0.0916 | ETputs(KTEPS) 2182.63\n",
      "Epoch 00139 | Loss 0.4422 | Time(s) 0.0915 | ETputs(KTEPS) 2186.98\n",
      "Epoch 00140 | Loss 0.4418 | Time(s) 0.0913 | ETputs(KTEPS) 2191.58\n",
      "Epoch 00141 | Loss 0.4414 | Time(s) 0.0911 | ETputs(KTEPS) 2196.15\n",
      "Epoch 00142 | Loss 0.4411 | Time(s) 0.0909 | ETputs(KTEPS) 2200.91\n",
      "Epoch 00143 | Loss 0.4408 | Time(s) 0.0907 | ETputs(KTEPS) 2205.02\n",
      "Epoch 00144 | Loss 0.4405 | Time(s) 0.0905 | ETputs(KTEPS) 2209.50\n",
      "Epoch 00145 | Loss 0.4402 | Time(s) 0.0903 | ETputs(KTEPS) 2213.80\n",
      "Epoch 00146 | Loss 0.4399 | Time(s) 0.0902 | ETputs(KTEPS) 2218.02\n",
      "Epoch 00147 | Loss 0.4396 | Time(s) 0.0900 | ETputs(KTEPS) 2222.16\n",
      "Epoch 00148 | Loss 0.4393 | Time(s) 0.0898 | ETputs(KTEPS) 2226.38\n",
      "Epoch 00149 | Loss 0.4390 | Time(s) 0.0897 | ETputs(KTEPS) 2230.51\n",
      "Epoch 00150 | Loss 0.4388 | Time(s) 0.0895 | ETputs(KTEPS) 2234.51\n",
      "Epoch 00151 | Loss 0.4385 | Time(s) 0.0893 | ETputs(KTEPS) 2238.76\n",
      "Epoch 00152 | Loss 0.4382 | Time(s) 0.0892 | ETputs(KTEPS) 2242.63\n",
      "Epoch 00153 | Loss 0.4380 | Time(s) 0.0890 | ETputs(KTEPS) 2247.02\n",
      "Epoch 00154 | Loss 0.4378 | Time(s) 0.0888 | ETputs(KTEPS) 2251.20\n",
      "Epoch 00155 | Loss 0.4375 | Time(s) 0.0887 | ETputs(KTEPS) 2255.17\n",
      "Epoch 00156 | Loss 0.4373 | Time(s) 0.0885 | ETputs(KTEPS) 2259.44\n",
      "Epoch 00157 | Loss 0.4371 | Time(s) 0.0884 | ETputs(KTEPS) 2263.56\n",
      "Epoch 00158 | Loss 0.4369 | Time(s) 0.0882 | ETputs(KTEPS) 2267.66\n",
      "Epoch 00159 | Loss 0.4367 | Time(s) 0.0880 | ETputs(KTEPS) 2272.08\n",
      "Epoch 00160 | Loss 0.4365 | Time(s) 0.0879 | ETputs(KTEPS) 2276.20\n",
      "Epoch 00161 | Loss 0.4363 | Time(s) 0.0877 | ETputs(KTEPS) 2280.31\n",
      "Epoch 00162 | Loss 0.4361 | Time(s) 0.0875 | ETputs(KTEPS) 2284.44\n",
      "Epoch 00163 | Loss 0.4360 | Time(s) 0.0878 | ETputs(KTEPS) 2278.05\n",
      "Epoch 00164 | Loss 0.4358 | Time(s) 0.0881 | ETputs(KTEPS) 2269.70\n",
      "Epoch 00165 | Loss 0.4356 | Time(s) 0.0884 | ETputs(KTEPS) 2261.59\n",
      "Epoch 00166 | Loss 0.4355 | Time(s) 0.0887 | ETputs(KTEPS) 2253.59\n",
      "Epoch 00167 | Loss 0.4353 | Time(s) 0.0891 | ETputs(KTEPS) 2245.75\n",
      "Epoch 00168 | Loss 0.4352 | Time(s) 0.0894 | ETputs(KTEPS) 2238.02\n",
      "Epoch 00169 | Loss 0.4350 | Time(s) 0.0897 | ETputs(KTEPS) 2230.45\n",
      "Epoch 00170 | Loss 0.4349 | Time(s) 0.0900 | ETputs(KTEPS) 2223.03\n",
      "Epoch 00171 | Loss 0.4347 | Time(s) 0.0903 | ETputs(KTEPS) 2215.75\n",
      "Epoch 00172 | Loss 0.4346 | Time(s) 0.0906 | ETputs(KTEPS) 2208.60\n",
      "Epoch 00173 | Loss 0.4345 | Time(s) 0.0908 | ETputs(KTEPS) 2201.56\n",
      "Epoch 00174 | Loss 0.4343 | Time(s) 0.0907 | ETputs(KTEPS) 2205.66\n",
      "Epoch 00175 | Loss 0.4342 | Time(s) 0.0904 | ETputs(KTEPS) 2212.07\n",
      "Epoch 00176 | Loss 0.4341 | Time(s) 0.0902 | ETputs(KTEPS) 2218.41\n",
      "Epoch 00177 | Loss 0.4340 | Time(s) 0.0899 | ETputs(KTEPS) 2224.76\n",
      "Epoch 00178 | Loss 0.4339 | Time(s) 0.0896 | ETputs(KTEPS) 2231.04\n",
      "Epoch 00179 | Loss 0.4338 | Time(s) 0.0894 | ETputs(KTEPS) 2237.25\n",
      "Epoch 00180 | Loss 0.4337 | Time(s) 0.0891 | ETputs(KTEPS) 2243.49\n",
      "Epoch 00181 | Loss 0.4336 | Time(s) 0.0889 | ETputs(KTEPS) 2249.71\n",
      "Epoch 00182 | Loss 0.4335 | Time(s) 0.0887 | ETputs(KTEPS) 2255.86\n",
      "Epoch 00183 | Loss 0.4334 | Time(s) 0.0884 | ETputs(KTEPS) 2261.96\n",
      "Epoch 00184 | Loss 0.4333 | Time(s) 0.0882 | ETputs(KTEPS) 2268.09\n",
      "Epoch 00185 | Loss 0.4332 | Time(s) 0.0879 | ETputs(KTEPS) 2274.15\n",
      "Epoch 00186 | Loss 0.4331 | Time(s) 0.0877 | ETputs(KTEPS) 2280.14\n",
      "Epoch 00187 | Loss 0.4330 | Time(s) 0.0875 | ETputs(KTEPS) 2286.13\n",
      "Epoch 00188 | Loss 0.4330 | Time(s) 0.0873 | ETputs(KTEPS) 2292.10\n",
      "Epoch 00189 | Loss 0.4329 | Time(s) 0.0870 | ETputs(KTEPS) 2298.03\n",
      "Epoch 00190 | Loss 0.4328 | Time(s) 0.0868 | ETputs(KTEPS) 2303.87\n",
      "Epoch 00191 | Loss 0.4327 | Time(s) 0.0866 | ETputs(KTEPS) 2309.79\n",
      "Epoch 00192 | Loss 0.4327 | Time(s) 0.0864 | ETputs(KTEPS) 2315.60\n",
      "Epoch 00193 | Loss 0.4326 | Time(s) 0.0862 | ETputs(KTEPS) 2321.36\n",
      "Epoch 00194 | Loss 0.4325 | Time(s) 0.0859 | ETputs(KTEPS) 2327.11\n",
      "Epoch 00195 | Loss 0.4325 | Time(s) 0.0857 | ETputs(KTEPS) 2332.85\n",
      "Epoch 00196 | Loss 0.4324 | Time(s) 0.0855 | ETputs(KTEPS) 2338.53\n",
      "Epoch 00197 | Loss 0.4324 | Time(s) 0.0853 | ETputs(KTEPS) 2344.16\n",
      "Epoch 00198 | Loss 0.4323 | Time(s) 0.0851 | ETputs(KTEPS) 2349.81\n",
      "Epoch 00199 | Loss 0.4323 | Time(s) 0.0849 | ETputs(KTEPS) 2355.44\n"
     ]
    }
   ],
   "source": [
    "# use optimizer\n",
    "trainer = gluon.Trainer(\n",
    "    quad_dense.collect_params(),\n",
    "    'adam',\n",
    "    {'learning_rate': 0.1, 'wd': 1e-4,})\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "for epoch in range(args.n_epochs):\n",
    "    t0 = time.time()\n",
    "    # forward\n",
    "    G.ndata['h'] = G.ndata['degree'] / (\n",
    "        G.ndata['degree'].sum(axis=1, keepdims=True) + 1e-10)\n",
    "    with mx.autograd.record():\n",
    "        G.apply_edges(quad_dense)\n",
    "        pred = G.edata.pop('pred').reshape((-1,))\n",
    "        loss = l2_loss(pred, G.edata['r'], G.edata['is_train'])\n",
    "\n",
    "    avg_loss = (loss.sum() / G.edata['is_train'].sum()).asscalar()\n",
    "    \n",
    "    loss.backward()\n",
    "    trainer.step(len(G))\n",
    "\n",
    "    dur.append(time.time() - t0)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f} | ETputs(KTEPS) {:.2f}\".format(\n",
    "        epoch, avg_loss, np.mean(dur), len(G.edges) / np.mean(dur) / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error 0.929786292205\n"
     ]
    }
   ],
   "source": [
    "print('training error', np.sqrt(avg_loss*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing error 0.959718942642\n"
     ]
    }
   ],
   "source": [
    "print('testing error', (\n",
    "    2. * l2_loss(pred, G.edata['r'], G.edata['is_test']).sum() /\n",
    "    G.edata['is_test'].sum()\n",
    ").sqrt().asscalar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_msg(edge, normalization=None):\n",
    "    # n_edges, n_classes\n",
    "    train_r = mx.nd.broadcast_mul(\n",
    "        mx.nd.one_hot(edge.data['r'], n_classes),\n",
    "        edge.data['is_train'].expand_dims(1)\n",
    "    )\n",
    "    # n_edges, n_classes, n_hidden\n",
    "    msg = mx.nd.broadcast_mul(\n",
    "        edge.src['h'],\n",
    "        train_r.expand_dims(2)\n",
    "    )\n",
    "    if normalization == 'sym':\n",
    "        msg = mx.nd.broadcast_div(\n",
    "            msg,\n",
    "            (edge.src['degree'] + 1e-10).sqrt().expand_dims(2)\n",
    "        )\n",
    "    return {'m': msg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_reduce(node, normalization=None):\n",
    "    # n_nodes, n_classes, n_hidden\n",
    "    accum = mx.nd.sum(node.mailbox['m'], axis=1)\n",
    "    if normalization == 'sym':\n",
    "        accum = mx.nd.broadcast_div(\n",
    "            accum,\n",
    "            (node.data['degree'] + 1e-10).sqrt().expand_dims(2)\n",
    "        )\n",
    "    elif normalization == 'left':\n",
    "        accum = mx.nd.broadcast_div(\n",
    "            accum,\n",
    "            (node.data['degree'] + 1e-10).expand_dims(2)\n",
    "        )\n",
    "    return {'accum' : accum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeUpdateModule(gluon.Block):\n",
    "    def __init__(self, out_feats, activation=None, dropout=0):\n",
    "        super(NodeUpdateModule, self).__init__()\n",
    "        self.linear = gluon.nn.Dense(out_feats,\n",
    "                                     activation=activation,\n",
    "                                     flatten=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, node):\n",
    "        accum = self.linear(node.data['accum'])\n",
    "        if self.dropout:\n",
    "            accum = mx.nd.Dropout(accum, p=self.dropout)\n",
    "        return {'h': node.data['h'] + accum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShareWeights(gluon.HybridBlock):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ShareWeights, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.triu = self.params.get_constant(\n",
    "                'triu',\n",
    "                np.triu(np.ones((n_classes, n_classes))))\n",
    "            \n",
    "    def hybrid_forward(self, F, batch_class_hidden, triu):\n",
    "        return F.dot(\n",
    "            batch_class_hidden.swapaxes(1,2),\n",
    "            triu\n",
    "        ).swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCMC(gluon.Block):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 normalization,\n",
    "                 share_weights=True,\n",
    "                 ):\n",
    "        super(GCMC, self).__init__()\n",
    "        self.n_hidden   = n_hidden\n",
    "        self.n_classes  = n_classes\n",
    "        self.dropout    = dropout\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.inp_layer  = gluon.nn.Embedding(\n",
    "                in_feats, n_classes * n_hidden)\n",
    "            if share_weights:\n",
    "                self.share_weights = ShareWeights(n_classes)\n",
    "            else:\n",
    "                self.share_weights = gluon.contrib.nn.Identity()\n",
    "\n",
    "            self.gcn_msg    = partial(gcn_msg, normalization=normalization)\n",
    "            self.gcn_reduce = partial(gcn_reduce, normalization=normalization)\n",
    "            \n",
    "            self.conv_layers = gluon.nn.Sequential()\n",
    "            for i in range(n_layers):\n",
    "                self.conv_layers.add(\n",
    "                    NodeUpdateModule(n_hidden, activation, dropout))\n",
    "\n",
    "            self.quad_fcn   = QuadDense()\n",
    "\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        emb_inp = self.inp_layer(features)\n",
    "        emb_inp = self.share_weights(\n",
    "            emb_inp\n",
    "            .reshape((-1, self.n_classes, self.n_hidden))\n",
    "        )\n",
    "        if self.dropout:\n",
    "            emb_inp = mx.nd.Dropout(emb_inp, p=self.dropout)\n",
    "        g.ndata['h'] = emb_inp\n",
    "        for layer in self.conv_layers:\n",
    "            g.update_all(self.gcn_msg, self.gcn_reduce, layer)\n",
    "\n",
    "        g.apply_edges(self.quad_fcn)\n",
    "\n",
    "        return g.edata.pop('pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCMC(len(G), args.n_hidden, n_classes, args.n_layers, 'relu', args.dropout, args.normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCMC(\n",
       "  (inp_layer): Embedding(2625 -> 36, float32)\n",
       "  (share_weights): ShareWeights(\n",
       "  \n",
       "  )\n",
       "  (conv_layers): Sequential(\n",
       "    (0): NodeUpdateModule(\n",
       "      (linear): Dense(None -> 6, Activation(relu))\n",
       "    )\n",
       "  )\n",
       "  (quad_fcn): QuadDense(\n",
       "    (dense): Dense(None -> 1, linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.collect_params().initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gcmc0_ (\n",
       "  Parameter gcmc0_embedding0_weight (shape=(2625, 36), dtype=float32)\n",
       "  Constant gcmc0_shareweights0_triu (shape=(6, 6), dtype=<class 'numpy.float32'>)\n",
       "  Parameter gcmc0_dense0_weight (shape=(6, 0), dtype=float32)\n",
       "  Parameter gcmc0_dense0_bias (shape=(6,), dtype=float32)\n",
       "  Parameter gcmc0_dense1_weight (shape=(1, 0), dtype=float32)\n",
       "  Parameter gcmc0_dense1_bias (shape=(1,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mx.nd.arange(len(G), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_edges = len(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 6.8678 | Time(s) 5.2667 | ETputs(KTEPS) 37.97\n",
      "Epoch 00001 | Loss 6.7679 | Time(s) 4.2807 | ETputs(KTEPS) 46.72\n",
      "Epoch 00002 | Loss 6.6272 | Time(s) 3.6470 | ETputs(KTEPS) 54.84\n",
      "Epoch 00003 | Loss 6.4376 | Time(s) 3.9382 | ETputs(KTEPS) 50.78\n",
      "Epoch 00004 | Loss 6.1814 | Time(s) 3.8315 | ETputs(KTEPS) 52.20\n",
      "Epoch 00005 | Loss 5.8688 | Time(s) 3.7398 | ETputs(KTEPS) 53.48\n",
      "Epoch 00006 | Loss 5.4921 | Time(s) 3.3636 | ETputs(KTEPS) 59.46\n",
      "Epoch 00007 | Loss 5.0301 | Time(s) 3.7345 | ETputs(KTEPS) 53.56\n",
      "Epoch 00008 | Loss 4.5077 | Time(s) 3.6316 | ETputs(KTEPS) 55.07\n",
      "Epoch 00009 | Loss 3.9203 | Time(s) 3.5177 | ETputs(KTEPS) 56.85\n",
      "Epoch 00010 | Loss 3.2853 | Time(s) 3.6667 | ETputs(KTEPS) 54.54\n",
      "Epoch 00011 | Loss 2.6410 | Time(s) 3.6249 | ETputs(KTEPS) 55.17\n",
      "Epoch 00012 | Loss 1.9857 | Time(s) 3.5916 | ETputs(KTEPS) 55.69\n",
      "Epoch 00013 | Loss 1.4421 | Time(s) 3.4130 | ETputs(KTEPS) 58.60\n",
      "Epoch 00014 | Loss 0.9923 | Time(s) 3.6047 | ETputs(KTEPS) 55.48\n",
      "Epoch 00015 | Loss 0.6931 | Time(s) 3.5581 | ETputs(KTEPS) 56.21\n",
      "Epoch 00016 | Loss 0.6528 | Time(s) 3.4926 | ETputs(KTEPS) 57.26\n",
      "Epoch 00017 | Loss 0.8215 | Time(s) 3.5389 | ETputs(KTEPS) 56.51\n",
      "Epoch 00018 | Loss 1.1167 | Time(s) 3.5514 | ETputs(KTEPS) 56.32\n",
      "Epoch 00019 | Loss 1.3389 | Time(s) 3.5447 | ETputs(KTEPS) 56.42\n",
      "Epoch 00020 | Loss 1.4228 | Time(s) 3.4281 | ETputs(KTEPS) 58.34\n",
      "Epoch 00021 | Loss 1.3079 | Time(s) 3.5579 | ETputs(KTEPS) 56.21\n",
      "Epoch 00022 | Loss 1.1731 | Time(s) 3.5257 | ETputs(KTEPS) 56.73\n",
      "Epoch 00023 | Loss 0.9114 | Time(s) 3.4824 | ETputs(KTEPS) 57.43\n",
      "Epoch 00024 | Loss 0.6969 | Time(s) 3.5175 | ETputs(KTEPS) 56.86\n",
      "Epoch 00025 | Loss 0.5793 | Time(s) 3.5275 | ETputs(KTEPS) 56.70\n",
      "Epoch 00026 | Loss 0.5108 | Time(s) 3.5232 | ETputs(KTEPS) 56.77\n",
      "Epoch 00027 | Loss 0.5007 | Time(s) 3.4366 | ETputs(KTEPS) 58.20\n",
      "Epoch 00028 | Loss 0.5345 | Time(s) 3.5350 | ETputs(KTEPS) 56.58\n",
      "Epoch 00029 | Loss 0.5718 | Time(s) 3.5113 | ETputs(KTEPS) 56.96\n",
      "Epoch 00030 | Loss 0.6331 | Time(s) 3.4780 | ETputs(KTEPS) 57.50\n",
      "Epoch 00031 | Loss 0.6742 | Time(s) 3.5054 | ETputs(KTEPS) 57.05\n",
      "Epoch 00032 | Loss 0.6710 | Time(s) 3.5137 | ETputs(KTEPS) 56.92\n",
      "Epoch 00033 | Loss 0.6690 | Time(s) 3.5112 | ETputs(KTEPS) 56.96\n",
      "Epoch 00034 | Loss 0.6460 | Time(s) 3.4423 | ETputs(KTEPS) 58.10\n",
      "Epoch 00035 | Loss 0.6120 | Time(s) 3.5210 | ETputs(KTEPS) 56.80\n",
      "Epoch 00036 | Loss 0.5789 | Time(s) 3.5020 | ETputs(KTEPS) 57.11\n",
      "Epoch 00037 | Loss 0.5292 | Time(s) 3.4754 | ETputs(KTEPS) 57.55\n",
      "Epoch 00038 | Loss 0.4934 | Time(s) 3.5268 | ETputs(KTEPS) 56.71\n",
      "Epoch 00039 | Loss 0.4695 | Time(s) 3.5122 | ETputs(KTEPS) 56.94\n",
      "Epoch 00040 | Loss 0.4534 | Time(s) 3.5025 | ETputs(KTEPS) 57.10\n",
      "Epoch 00041 | Loss 0.4547 | Time(s) 3.4441 | ETputs(KTEPS) 58.07\n",
      "Epoch 00042 | Loss 0.4675 | Time(s) 3.5103 | ETputs(KTEPS) 56.98\n",
      "Epoch 00043 | Loss 0.4842 | Time(s) 3.4947 | ETputs(KTEPS) 57.23\n",
      "Epoch 00044 | Loss 0.4922 | Time(s) 3.4719 | ETputs(KTEPS) 57.61\n",
      "Epoch 00045 | Loss 0.4995 | Time(s) 3.4909 | ETputs(KTEPS) 57.29\n",
      "Epoch 00046 | Loss 0.5006 | Time(s) 3.4972 | ETputs(KTEPS) 57.19\n",
      "Epoch 00047 | Loss 0.4958 | Time(s) 3.4954 | ETputs(KTEPS) 57.22\n",
      "Epoch 00048 | Loss 0.4810 | Time(s) 3.4470 | ETputs(KTEPS) 58.02\n",
      "Epoch 00049 | Loss 0.4682 | Time(s) 3.5036 | ETputs(KTEPS) 57.08\n",
      "Epoch 00050 | Loss 0.4539 | Time(s) 3.4901 | ETputs(KTEPS) 57.31\n",
      "Epoch 00051 | Loss 0.4465 | Time(s) 3.4711 | ETputs(KTEPS) 57.62\n",
      "Epoch 00052 | Loss 0.4442 | Time(s) 3.4876 | ETputs(KTEPS) 57.35\n",
      "Epoch 00053 | Loss 0.4477 | Time(s) 3.4927 | ETputs(KTEPS) 57.26\n",
      "Epoch 00054 | Loss 0.4511 | Time(s) 3.4912 | ETputs(KTEPS) 57.29\n",
      "Epoch 00055 | Loss 0.4547 | Time(s) 3.4486 | ETputs(KTEPS) 57.99\n",
      "Epoch 00056 | Loss 0.4478 | Time(s) 3.4984 | ETputs(KTEPS) 57.17\n",
      "Epoch 00057 | Loss 0.4556 | Time(s) 3.4864 | ETputs(KTEPS) 57.37\n",
      "Epoch 00058 | Loss 0.4544 | Time(s) 3.4691 | ETputs(KTEPS) 57.65\n",
      "Epoch 00059 | Loss 0.4521 | Time(s) 3.4842 | ETputs(KTEPS) 57.40\n",
      "Epoch 00060 | Loss 0.4525 | Time(s) 3.4891 | ETputs(KTEPS) 57.32\n",
      "Epoch 00061 | Loss 0.4428 | Time(s) 3.4880 | ETputs(KTEPS) 57.34\n",
      "Epoch 00062 | Loss 0.4456 | Time(s) 3.4499 | ETputs(KTEPS) 57.97\n",
      "Epoch 00063 | Loss 0.4402 | Time(s) 3.4944 | ETputs(KTEPS) 57.24\n",
      "Epoch 00064 | Loss 0.4395 | Time(s) 3.4839 | ETputs(KTEPS) 57.41\n",
      "Epoch 00065 | Loss 0.4419 | Time(s) 3.4685 | ETputs(KTEPS) 57.66\n",
      "Epoch 00066 | Loss 0.4425 | Time(s) 3.4818 | ETputs(KTEPS) 57.44\n",
      "Epoch 00067 | Loss 0.4417 | Time(s) 3.4862 | ETputs(KTEPS) 57.37\n",
      "Epoch 00068 | Loss 0.4450 | Time(s) 3.4854 | ETputs(KTEPS) 57.38\n",
      "Epoch 00069 | Loss 0.4455 | Time(s) 3.4511 | ETputs(KTEPS) 57.95\n",
      "Epoch 00070 | Loss 0.4456 | Time(s) 3.4910 | ETputs(KTEPS) 57.29\n",
      "Epoch 00071 | Loss 0.4419 | Time(s) 3.4817 | ETputs(KTEPS) 57.44\n",
      "Epoch 00072 | Loss 0.4407 | Time(s) 3.4681 | ETputs(KTEPS) 57.67\n",
      "Epoch 00073 | Loss 0.4408 | Time(s) 3.4892 | ETputs(KTEPS) 57.32\n",
      "Epoch 00074 | Loss 0.4363 | Time(s) 3.4861 | ETputs(KTEPS) 57.37\n",
      "Epoch 00075 | Loss 0.4389 | Time(s) 3.4829 | ETputs(KTEPS) 57.42\n",
      "Epoch 00076 | Loss 0.4376 | Time(s) 3.4517 | ETputs(KTEPS) 57.94\n",
      "Epoch 00077 | Loss 0.4395 | Time(s) 3.4879 | ETputs(KTEPS) 57.34\n",
      "Epoch 00078 | Loss 0.4395 | Time(s) 3.4793 | ETputs(KTEPS) 57.48\n",
      "Epoch 00079 | Loss 0.4400 | Time(s) 3.4668 | ETputs(KTEPS) 57.69\n",
      "Epoch 00080 | Loss 0.4379 | Time(s) 3.4777 | ETputs(KTEPS) 57.51\n",
      "Epoch 00081 | Loss 0.4387 | Time(s) 3.4814 | ETputs(KTEPS) 57.45\n",
      "Epoch 00082 | Loss 0.4386 | Time(s) 3.4806 | ETputs(KTEPS) 57.46\n",
      "Epoch 00083 | Loss 0.4399 | Time(s) 3.4525 | ETputs(KTEPS) 57.93\n",
      "Epoch 00084 | Loss 0.4388 | Time(s) 3.4856 | ETputs(KTEPS) 57.38\n",
      "Epoch 00085 | Loss 0.4383 | Time(s) 3.4777 | ETputs(KTEPS) 57.51\n",
      "Epoch 00086 | Loss 0.4368 | Time(s) 3.4664 | ETputs(KTEPS) 57.70\n",
      "Epoch 00087 | Loss 0.4389 | Time(s) 3.4765 | ETputs(KTEPS) 57.53\n",
      "Epoch 00088 | Loss 0.4357 | Time(s) 3.4799 | ETputs(KTEPS) 57.47\n",
      "Epoch 00089 | Loss 0.4369 | Time(s) 3.4791 | ETputs(KTEPS) 57.49\n",
      "Epoch 00090 | Loss 0.4364 | Time(s) 3.4531 | ETputs(KTEPS) 57.92\n",
      "Epoch 00091 | Loss 0.4389 | Time(s) 3.4839 | ETputs(KTEPS) 57.41\n",
      "Epoch 00092 | Loss 0.4382 | Time(s) 3.4765 | ETputs(KTEPS) 57.53\n",
      "Epoch 00093 | Loss 0.4377 | Time(s) 3.4659 | ETputs(KTEPS) 57.70\n",
      "Epoch 00094 | Loss 0.4387 | Time(s) 3.4752 | ETputs(KTEPS) 57.55\n",
      "Epoch 00095 | Loss 0.4394 | Time(s) 3.4783 | ETputs(KTEPS) 57.50\n",
      "Epoch 00096 | Loss 0.4382 | Time(s) 3.4776 | ETputs(KTEPS) 57.51\n",
      "Epoch 00097 | Loss 0.4373 | Time(s) 3.4533 | ETputs(KTEPS) 57.92\n",
      "Epoch 00098 | Loss 0.4375 | Time(s) 3.4819 | ETputs(KTEPS) 57.44\n",
      "Epoch 00099 | Loss 0.4388 | Time(s) 3.4753 | ETputs(KTEPS) 57.55\n",
      "Epoch 00100 | Loss 0.4363 | Time(s) 3.4655 | ETputs(KTEPS) 57.71\n",
      "Epoch 00101 | Loss 0.4386 | Time(s) 3.4743 | ETputs(KTEPS) 57.57\n",
      "Epoch 00102 | Loss 0.4376 | Time(s) 3.4774 | ETputs(KTEPS) 57.51\n",
      "Epoch 00103 | Loss 0.4365 | Time(s) 3.4769 | ETputs(KTEPS) 57.52\n",
      "Epoch 00104 | Loss 0.4367 | Time(s) 3.4545 | ETputs(KTEPS) 57.90\n",
      "Epoch 00105 | Loss 0.4360 | Time(s) 3.4809 | ETputs(KTEPS) 57.46\n",
      "Epoch 00106 | Loss 0.4406 | Time(s) 3.4748 | ETputs(KTEPS) 57.56\n",
      "Epoch 00107 | Loss 0.4385 | Time(s) 3.4655 | ETputs(KTEPS) 57.71\n",
      "Epoch 00108 | Loss 0.4379 | Time(s) 3.4740 | ETputs(KTEPS) 57.57\n",
      "Epoch 00109 | Loss 0.4368 | Time(s) 3.4765 | ETputs(KTEPS) 57.53\n",
      "Epoch 00110 | Loss 0.4367 | Time(s) 3.4760 | ETputs(KTEPS) 57.54\n",
      "Epoch 00111 | Loss 0.4381 | Time(s) 3.4550 | ETputs(KTEPS) 57.89\n",
      "Epoch 00112 | Loss 0.4368 | Time(s) 3.4801 | ETputs(KTEPS) 57.47\n",
      "Epoch 00113 | Loss 0.4395 | Time(s) 3.4742 | ETputs(KTEPS) 57.57\n",
      "Epoch 00114 | Loss 0.4376 | Time(s) 3.4656 | ETputs(KTEPS) 57.71\n",
      "Epoch 00115 | Loss 0.4367 | Time(s) 3.4733 | ETputs(KTEPS) 57.58\n",
      "Epoch 00116 | Loss 0.4374 | Time(s) 3.4755 | ETputs(KTEPS) 57.55\n",
      "Epoch 00117 | Loss 0.4361 | Time(s) 3.4748 | ETputs(KTEPS) 57.56\n",
      "Epoch 00118 | Loss 0.4367 | Time(s) 3.4550 | ETputs(KTEPS) 57.89\n",
      "Epoch 00119 | Loss 0.4372 | Time(s) 3.4785 | ETputs(KTEPS) 57.50\n",
      "Epoch 00120 | Loss 0.4386 | Time(s) 3.4730 | ETputs(KTEPS) 57.59\n",
      "Epoch 00121 | Loss 0.4363 | Time(s) 3.4650 | ETputs(KTEPS) 57.72\n",
      "Epoch 00122 | Loss 0.4369 | Time(s) 3.4721 | ETputs(KTEPS) 57.60\n",
      "Epoch 00123 | Loss 0.4360 | Time(s) 3.4744 | ETputs(KTEPS) 57.56\n",
      "Epoch 00124 | Loss 0.4368 | Time(s) 3.4740 | ETputs(KTEPS) 57.57\n",
      "Epoch 00125 | Loss 0.4349 | Time(s) 3.4550 | ETputs(KTEPS) 57.89\n",
      "Epoch 00126 | Loss 0.4380 | Time(s) 3.4774 | ETputs(KTEPS) 57.51\n",
      "Epoch 00127 | Loss 0.4360 | Time(s) 3.4722 | ETputs(KTEPS) 57.60\n",
      "Epoch 00128 | Loss 0.4364 | Time(s) 3.4643 | ETputs(KTEPS) 57.73\n",
      "Epoch 00129 | Loss 0.4367 | Time(s) 3.4711 | ETputs(KTEPS) 57.62\n",
      "Epoch 00130 | Loss 0.4381 | Time(s) 3.4735 | ETputs(KTEPS) 57.58\n",
      "Epoch 00131 | Loss 0.4351 | Time(s) 3.4731 | ETputs(KTEPS) 57.58\n",
      "Epoch 00132 | Loss 0.4344 | Time(s) 3.4554 | ETputs(KTEPS) 57.88\n",
      "Epoch 00133 | Loss 0.4363 | Time(s) 3.4767 | ETputs(KTEPS) 57.53\n",
      "Epoch 00134 | Loss 0.4393 | Time(s) 3.4719 | ETputs(KTEPS) 57.61\n",
      "Epoch 00135 | Loss 0.4373 | Time(s) 3.4643 | ETputs(KTEPS) 57.73\n",
      "Epoch 00136 | Loss 0.4385 | Time(s) 3.4708 | ETputs(KTEPS) 57.62\n",
      "Epoch 00137 | Loss 0.4397 | Time(s) 3.4732 | ETputs(KTEPS) 57.58\n",
      "Epoch 00138 | Loss 0.4357 | Time(s) 3.4729 | ETputs(KTEPS) 57.59\n",
      "Epoch 00139 | Loss 0.4366 | Time(s) 3.4559 | ETputs(KTEPS) 57.87\n",
      "Epoch 00140 | Loss 0.4376 | Time(s) 3.4760 | ETputs(KTEPS) 57.54\n",
      "Epoch 00141 | Loss 0.4391 | Time(s) 3.4713 | ETputs(KTEPS) 57.62\n",
      "Epoch 00142 | Loss 0.4367 | Time(s) 3.4643 | ETputs(KTEPS) 57.73\n",
      "Epoch 00143 | Loss 0.4366 | Time(s) 3.4706 | ETputs(KTEPS) 57.63\n",
      "Epoch 00144 | Loss 0.4381 | Time(s) 3.4726 | ETputs(KTEPS) 57.59\n",
      "Epoch 00145 | Loss 0.4361 | Time(s) 3.4722 | ETputs(KTEPS) 57.60\n",
      "Epoch 00146 | Loss 0.4374 | Time(s) 3.4561 | ETputs(KTEPS) 57.87\n",
      "Epoch 00147 | Loss 0.4370 | Time(s) 3.4753 | ETputs(KTEPS) 57.55\n",
      "Epoch 00148 | Loss 0.4371 | Time(s) 3.4708 | ETputs(KTEPS) 57.62\n",
      "Epoch 00149 | Loss 0.4370 | Time(s) 3.4643 | ETputs(KTEPS) 57.73\n",
      "Epoch 00150 | Loss 0.4353 | Time(s) 3.4702 | ETputs(KTEPS) 57.63\n",
      "Epoch 00151 | Loss 0.4368 | Time(s) 3.4724 | ETputs(KTEPS) 57.60\n",
      "Epoch 00152 | Loss 0.4357 | Time(s) 3.4720 | ETputs(KTEPS) 57.60\n",
      "Epoch 00153 | Loss 0.4358 | Time(s) 3.4567 | ETputs(KTEPS) 57.86\n",
      "Epoch 00154 | Loss 0.4370 | Time(s) 3.4751 | ETputs(KTEPS) 57.55\n",
      "Epoch 00155 | Loss 0.4379 | Time(s) 3.4708 | ETputs(KTEPS) 57.62\n",
      "Epoch 00156 | Loss 0.4361 | Time(s) 3.4644 | ETputs(KTEPS) 57.73\n",
      "Epoch 00157 | Loss 0.4352 | Time(s) 3.4701 | ETputs(KTEPS) 57.64\n",
      "Epoch 00158 | Loss 0.4386 | Time(s) 3.4720 | ETputs(KTEPS) 57.60\n",
      "Epoch 00159 | Loss 0.4386 | Time(s) 3.4715 | ETputs(KTEPS) 57.61\n",
      "Epoch 00160 | Loss 0.4357 | Time(s) 3.4569 | ETputs(KTEPS) 57.85\n",
      "Epoch 00161 | Loss 0.4363 | Time(s) 3.4393 | ETputs(KTEPS) 58.15\n",
      "Epoch 00162 | Loss 0.4353 | Time(s) 3.4220 | ETputs(KTEPS) 58.45\n",
      "Epoch 00163 | Loss 0.4367 | Time(s) 3.4049 | ETputs(KTEPS) 58.74\n",
      "Epoch 00164 | Loss 0.4340 | Time(s) 3.3881 | ETputs(KTEPS) 59.03\n",
      "Epoch 00165 | Loss 0.4365 | Time(s) 3.3713 | ETputs(KTEPS) 59.32\n",
      "Epoch 00166 | Loss 0.4363 | Time(s) 3.3548 | ETputs(KTEPS) 59.62\n",
      "Epoch 00167 | Loss 0.4372 | Time(s) 3.3384 | ETputs(KTEPS) 59.91\n",
      "Epoch 00168 | Loss 0.4357 | Time(s) 3.3223 | ETputs(KTEPS) 60.20\n",
      "Epoch 00169 | Loss 0.4359 | Time(s) 3.3064 | ETputs(KTEPS) 60.49\n",
      "Epoch 00170 | Loss 0.4357 | Time(s) 3.2907 | ETputs(KTEPS) 60.78\n",
      "Epoch 00171 | Loss 0.4352 | Time(s) 3.2751 | ETputs(KTEPS) 61.07\n",
      "Epoch 00172 | Loss 0.4364 | Time(s) 3.2597 | ETputs(KTEPS) 61.36\n",
      "Epoch 00173 | Loss 0.4371 | Time(s) 3.2445 | ETputs(KTEPS) 61.64\n",
      "Epoch 00174 | Loss 0.4352 | Time(s) 3.2296 | ETputs(KTEPS) 61.93\n",
      "Epoch 00175 | Loss 0.4357 | Time(s) 3.2147 | ETputs(KTEPS) 62.21\n",
      "Epoch 00176 | Loss 0.4391 | Time(s) 3.2000 | ETputs(KTEPS) 62.50\n",
      "Epoch 00177 | Loss 0.4359 | Time(s) 3.1855 | ETputs(KTEPS) 62.79\n",
      "Epoch 00178 | Loss 0.4346 | Time(s) 3.1711 | ETputs(KTEPS) 63.07\n",
      "Epoch 00179 | Loss 0.4334 | Time(s) 3.1569 | ETputs(KTEPS) 63.35\n",
      "Epoch 00180 | Loss 0.4343 | Time(s) 3.1429 | ETputs(KTEPS) 63.64\n",
      "Epoch 00181 | Loss 0.4384 | Time(s) 3.1290 | ETputs(KTEPS) 63.92\n",
      "Epoch 00182 | Loss 0.4354 | Time(s) 3.1153 | ETputs(KTEPS) 64.20\n",
      "Epoch 00183 | Loss 0.4352 | Time(s) 3.1017 | ETputs(KTEPS) 64.48\n",
      "Epoch 00184 | Loss 0.4348 | Time(s) 3.0882 | ETputs(KTEPS) 64.76\n",
      "Epoch 00185 | Loss 0.4339 | Time(s) 3.0750 | ETputs(KTEPS) 65.04\n",
      "Epoch 00186 | Loss 0.4362 | Time(s) 3.0618 | ETputs(KTEPS) 65.32\n",
      "Epoch 00187 | Loss 0.4363 | Time(s) 3.0488 | ETputs(KTEPS) 65.60\n",
      "Epoch 00188 | Loss 0.4371 | Time(s) 3.0359 | ETputs(KTEPS) 65.88\n",
      "Epoch 00189 | Loss 0.4361 | Time(s) 3.0231 | ETputs(KTEPS) 66.16\n",
      "Epoch 00190 | Loss 0.4379 | Time(s) 3.0105 | ETputs(KTEPS) 66.43\n",
      "Epoch 00191 | Loss 0.4347 | Time(s) 2.9980 | ETputs(KTEPS) 66.71\n",
      "Epoch 00192 | Loss 0.4372 | Time(s) 2.9857 | ETputs(KTEPS) 66.99\n",
      "Epoch 00193 | Loss 0.4351 | Time(s) 2.9735 | ETputs(KTEPS) 67.26\n",
      "Epoch 00194 | Loss 0.4356 | Time(s) 2.9615 | ETputs(KTEPS) 67.53\n",
      "Epoch 00195 | Loss 0.4364 | Time(s) 2.9495 | ETputs(KTEPS) 67.81\n",
      "Epoch 00196 | Loss 0.4373 | Time(s) 2.9377 | ETputs(KTEPS) 68.08\n",
      "Epoch 00197 | Loss 0.4372 | Time(s) 2.9259 | ETputs(KTEPS) 68.35\n",
      "Epoch 00198 | Loss 0.4364 | Time(s) 2.9143 | ETputs(KTEPS) 68.63\n",
      "Epoch 00199 | Loss 0.4353 | Time(s) 2.9028 | ETputs(KTEPS) 68.90\n"
     ]
    }
   ],
   "source": [
    "# use optimizer\n",
    "trainer = gluon.Trainer(\n",
    "    model.collect_params(),\n",
    "    'adam',\n",
    "    {'learning_rate': 0.01, 'wd': 1e-4,})\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "for epoch in range(args.n_epochs):\n",
    "    t0 = time.time()\n",
    "    # forward\n",
    "    with mx.autograd.record():\n",
    "        pred = model(G, features)\n",
    "        loss = loss_fcn(pred.reshape((-1,)), G.edata['r'], G.edata['is_train'])\n",
    "    avg_loss = (loss.sum() / G.edata['is_train'].sum()).asscalar()\n",
    "    \n",
    "    loss.backward()\n",
    "    trainer.step(len(G))\n",
    "\n",
    "    dur.append(time.time() - t0)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f} | ETputs(KTEPS) {:.2f}\".format(\n",
    "        epoch, avg_loss, np.mean(dur), n_edges / np.mean(dur) / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model(G, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.911303\n"
     ]
    }
   ],
   "source": [
    "print('training loss', (2. * l2_loss(\n",
    "    test_pred.reshape((-1,)),\n",
    "    G.edata['r'],\n",
    "    G.edata['is_train'],\n",
    ").sum() / G.edata['is_train'].sum()).sqrt().asscalar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss 0.946713\n"
     ]
    }
   ],
   "source": [
    "print('testing loss', (2. * l2_loss(\n",
    "    test_pred.reshape((-1,)),\n",
    "    G.edata['r'],\n",
    "    G.edata['is_test'],\n",
    ").sum() / G.edata['is_test'].sum()).sqrt().asscalar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
